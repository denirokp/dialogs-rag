#!/usr/bin/env python3
"""
üéØ –§–ò–ù–ê–õ–¨–ù–´–ô –ü–ê–ô–ü–õ–ê–ô–ù DIALOGS RAG SYSTEM
–ü–æ–ª–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –∞–Ω–∞–ª–∏–∑–∞ –¥–∏–∞–ª–æ–≥–æ–≤ —Å –æ–±—É—á–µ–Ω–∏–µ–º, A/B —Ç–µ—Å—Ç–∞–º–∏ –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º
"""

import asyncio
import json
import logging
import time
from pathlib import Path
from typing import Dict, List, Any, Optional
import pandas as pd
import numpy as np
from datetime import datetime
import argparse
import sys

# –ò–º–ø–æ—Ä—Ç –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ —Å–∏—Å—Ç–µ–º—ã
from enhanced.integrated_system import IntegratedQualitySystem
from enhanced.quality_autocorrection import QualityAutoCorrector
from enhanced.adaptive_prompts import AdaptivePromptSystem
from enhanced.continuous_learning import ContinuousLearningSystem
from enhanced.quality_monitoring import QualityMonitor
from enhanced.scaling_optimizer import ScalingOptimizer

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/final_pipeline.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class FinalPipeline:
    """–§–∏–Ω–∞–ª—å–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω —Å –ø–æ–ª–Ω—ã–º —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª–æ–º"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.results = []
        self.statistics = {}
        self.learning_data = []
        self.ab_test_results = {}
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        self.quality_system = None
        self.autocorrector = None
        self.adaptive_prompts = None
        self.learning_system = None
        self.monitor = None
        self.scaler = None
        
        self._initialize_components()
    
    def _initialize_components(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ —Å–∏—Å—Ç–µ–º—ã"""
        logger.info("üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞...")
        
        try:
            # 1. –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –∫–∞—á–µ—Å—Ç–≤–∞
            self.quality_system = IntegratedQualitySystem(self.config)
            logger.info("‚úÖ –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            
            # 2. –ê–≤—Ç–æ–∫–æ—Ä—Ä–µ–∫—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞
            self.autocorrector = QualityAutoCorrector(self.config)
            logger.info("‚úÖ –°–∏—Å—Ç–µ–º–∞ –∞–≤—Ç–æ–∫–æ—Ä—Ä–µ–∫—Ü–∏–∏ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            
            # 3. –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã —Å A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º
            self.adaptive_prompts = AdaptivePromptSystem(self.config)
            logger.info("‚úÖ –°–∏—Å—Ç–µ–º–∞ –∞–¥–∞–ø—Ç–∏–≤–Ω—ã—Ö –ø—Ä–æ–º–ø—Ç–æ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            
            # 4. –ù–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ
            self.learning_system = ContinuousLearningSystem(self.config)
            logger.info("‚úÖ –°–∏—Å—Ç–µ–º–∞ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            
            # 5. –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∫–∞—á–µ—Å—Ç–≤–∞
            self.monitor = QualityMonitor(self.config)
            logger.info("‚úÖ –°–∏—Å—Ç–µ–º–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            
            # 6. –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
            self.scaler = ScalingOptimizer(self.config)
            logger.info("‚úÖ –°–∏—Å—Ç–µ–º–∞ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            
            logger.info("üéâ –í—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã!")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏: {e}")
            raise
    
    async def process_dialogs(self, dialogs: List[str]) -> Dict[str, Any]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∏–∞–ª–æ–≥–æ–≤ —Å –ø–æ–ª–Ω—ã–º —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª–æ–º"""
        logger.info(f"üìä –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É {len(dialogs)} –¥–∏–∞–ª–æ–≥–æ–≤...")
        
        start_time = time.time()
        results = []
        learning_examples = []
        ab_test_data = []
        
        # –ü—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä
        from tqdm import tqdm
        
        for i, dialog in enumerate(tqdm(dialogs, desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∏–∞–ª–æ–≥–æ–≤")):
            try:
                # 1. A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–æ–≤
                prompt_variant = self.adaptive_prompts.select_variant("default_quality_test")
                
                # 2. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π —Å –≤—ã–±—Ä–∞–Ω–Ω—ã–º –ø—Ä–æ–º–ø—Ç–æ–º
                extraction_result = await self._extract_entities_with_prompt(dialog, prompt_variant)
                
                # 3. –ê–≤—Ç–æ–∫–æ—Ä—Ä–µ–∫—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞
                corrected_result = self.autocorrector.correct_extraction(extraction_result, dialog)
                
                # 4. –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
                quality_score = self._calculate_quality_score(corrected_result, dialog)
                
                # 5. –°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
                result = {
                    "dialog_id": i,
                    "dialog": dialog,
                    "extracted_entities": corrected_result.extracted_entities,
                    "quality_score": quality_score,
                    "prompt_variant": prompt_variant,
                    "processing_timestamp": datetime.now().isoformat(),
                    "corrections_applied": corrected_result.corrections_applied,
                    "ab_test_variant": prompt_variant,
                    "learning_quality": quality_score
                }
                
                results.append(result)
                
                # 6. –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤ –æ–±—É—á–µ–Ω–∏–µ
                if quality_score > 0.3:  # –¢–æ–ª—å–∫–æ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã
                    learning_example = {
                        "dialog": dialog,
                        "entities": corrected_result.extracted_entities,
                        "quality_score": quality_score,
                        "source": "final_pipeline",
                        "timestamp": datetime.now().isoformat()
                    }
                    learning_examples.append(learning_example)
                
                # 7. A/B —Ç–µ—Å—Ç –¥–∞–Ω–Ω—ã–µ
                ab_test_data.append({
                    "variant": prompt_variant,
                    "quality_score": quality_score,
                    "dialog_length": len(dialog),
                    "timestamp": datetime.now().isoformat()
                })
                
                # 8. –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
                self.monitor.record_processing_metrics({
                    "quality_score": quality_score,
                    "processing_time": time.time() - start_time,
                    "dialog_length": len(dialog)
                })
                
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∏–∞–ª–æ–≥–∞ {i}: {e}")
                # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å –æ—à–∏–±–∫–æ–π
                results.append({
                    "dialog_id": i,
                    "dialog": dialog,
                    "error": str(e),
                    "quality_score": 0.0,
                    "processing_timestamp": datetime.now().isoformat()
                })
        
        # 9. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ A/B —Ç–µ—Å—Ç–æ–≤
        for data in ab_test_data:
            self.adaptive_prompts.record_result("default_quality_test", data["variant"], data["quality_score"])
        
        # 10. –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤ –æ–±—É—á–µ–Ω–∏–µ
        for example in learning_examples:
            self.learning_system.add_learning_example(example)
        
        # 11. –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        analysis = self._analyze_results(results)
        
        # 12. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        processing_time = time.time() - start_time
        self.statistics = {
            "total_dialogs": len(dialogs),
            "processed_dialogs": len(results),
            "success_rate": len([r for r in results if "error" not in r]) / len(dialogs),
            "avg_quality_score": np.mean([r["quality_score"] for r in results if "error" not in r]),
            "processing_time_seconds": processing_time,
            "dialogs_per_second": len(dialogs) / processing_time,
            "ab_test_results": self.adaptive_prompts.get_ab_test_summary("default_quality_test"),
            "learning_examples_added": len(learning_examples),
            "monitoring_stats": self.monitor.get_processing_stats()
        }
        
        self.results = results
        self.learning_data = learning_examples
        
        logger.info(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {processing_time:.2f} —Å–µ–∫—É–Ω–¥")
        logger.info(f"üìà –°—Ä–µ–¥–Ω–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ: {self.statistics['avg_quality_score']:.3f}")
        logger.info(f"üéØ –£—Å–ø–µ—à–Ω–æ—Å—Ç—å: {self.statistics['success_rate']:.1%}")
        
        return {
            "results": results,
            "statistics": self.statistics,
            "analysis": analysis,
            "ab_test_results": self.ab_test_results
        }
    
    async def _extract_entities_with_prompt(self, dialog: str, prompt_variant: str) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞"""
        try:
            # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è –≤–∞—Ä–∏–∞–Ω—Ç–∞
            prompt = self.adaptive_prompts.get_prompt(prompt_variant)
            
            # –ó–¥–µ—Å—å –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –ª–æ–≥–∏–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—É—â–Ω–æ—Å—Ç–µ–π
            # –î–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ —Å–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ
            entities = {
                "problems": [],
                "ideas": [],
                "barriers": [],
                "quotes": []
            }
            
            # –ü—Ä–æ—Å—Ç–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ü–∏—Ç–∞—Ç
            if "–ø—Ä–æ–±–ª–µ–º–∞" in dialog.lower():
                entities["problems"].append("–û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –ø—Ä–æ–±–ª–µ–º–∞: –ø—Ä–æ–±–ª–µ–º–∞")
            
            if "–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ" in dialog.lower():
                entities["ideas"].append("–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ: –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ")
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ü–∏—Ç–∞—Ç (—É–ø—Ä–æ—â–µ–Ω–Ω–æ–µ)
            sentences = dialog.split('.')
            for sentence in sentences:
                if len(sentence.strip()) > 20 and any(word in sentence.lower() for word in ["–∫–ª–∏–µ–Ω—Ç", "–æ–ø–µ—Ä–∞—Ç–æ—Ä", "–º–µ–Ω–µ–¥–∂–µ—Ä"]):
                    entities["quotes"].append(sentence.strip())
            
            return {
                "extracted_entities": entities,
                "prompt_used": prompt_variant,
                "processing_time": 0.1
            }
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—É—â–Ω–æ—Å—Ç–µ–π: {e}")
            return {
                "extracted_entities": {"problems": [], "ideas": [], "barriers": [], "quotes": []},
                "prompt_used": prompt_variant,
                "error": str(e)
            }
    
    def _calculate_quality_score(self, result: Any, dialog: str) -> float:
        """–†–∞—Å—á–µ—Ç –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞"""
        try:
            if hasattr(result, 'quality_score'):
                return result.quality_score
            
            # –ü—Ä–æ—Å—Ç–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
            entities = result.extracted_entities if hasattr(result, 'extracted_entities') else {}
            
            score = 0.0
            
            # –û—Ü–µ–Ω–∫–∞ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π
            total_entities = sum(len(v) for v in entities.values() if isinstance(v, list))
            if total_entities > 0:
                score += min(0.5, total_entities * 0.1)
            
            # –û—Ü–µ–Ω–∫–∞ –ø–æ –¥–ª–∏–Ω–µ –¥–∏–∞–ª–æ–≥–∞
            if len(dialog) > 100:
                score += 0.2
            
            # –û—Ü–µ–Ω–∫–∞ –ø–æ –Ω–∞–ª–∏—á–∏—é –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            keywords = ["–ø—Ä–æ–±–ª–µ–º–∞", "–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ", "–¥–æ—Å—Ç–∞–≤–∫–∞", "–∑–∞–∫–∞–∑", "–∫–ª–∏–µ–Ω—Ç"]
            found_keywords = sum(1 for kw in keywords if kw in dialog.lower())
            score += min(0.3, found_keywords * 0.1)
            
            return min(1.0, score)
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ –∫–∞—á–µ—Å—Ç–≤–∞: {e}")
            return 0.0
    
    def _analyze_results(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        try:
            successful_results = [r for r in results if "error" not in r]
            
            if not successful_results:
                return {"error": "–ù–µ—Ç —É—Å–ø–µ—à–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"}
            
            # –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞
            quality_scores = [r["quality_score"] for r in successful_results]
            
            # –ê–Ω–∞–ª–∏–∑ —Å—É—â–Ω–æ—Å—Ç–µ–π
            all_entities = {"problems": [], "ideas": [], "barriers": [], "quotes": []}
            for result in successful_results:
                entities = result.get("extracted_entities", {})
                for key, values in entities.items():
                    if key in all_entities and isinstance(values, list):
                        all_entities[key].extend(values)
            
            # –ê–Ω–∞–ª–∏–∑ A/B —Ç–µ—Å—Ç–æ–≤
            ab_variants = {}
            for result in successful_results:
                variant = result.get("prompt_variant", "unknown")
                if variant not in ab_variants:
                    ab_variants[variant] = []
                ab_variants[variant].append(result["quality_score"])
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –≤–∞—Ä–∏–∞–Ω—Ç–∞–º
            variant_stats = {}
            for variant, scores in ab_variants.items():
                variant_stats[variant] = {
                    "count": len(scores),
                    "avg_quality": np.mean(scores),
                    "std_quality": np.std(scores),
                    "min_quality": np.min(scores),
                    "max_quality": np.max(scores)
                }
            
            analysis = {
                "quality_analysis": {
                    "avg_quality": np.mean(quality_scores),
                    "std_quality": np.std(quality_scores),
                    "min_quality": np.min(quality_scores),
                    "max_quality": np.max(quality_scores),
                    "quality_distribution": {
                        "high": len([s for s in quality_scores if s >= 0.7]),
                        "medium": len([s for s in quality_scores if 0.4 <= s < 0.7]),
                        "low": len([s for s in quality_scores if s < 0.4])
                    }
                },
                "entities_analysis": {
                    "total_problems": len(all_entities["problems"]),
                    "total_ideas": len(all_entities["ideas"]),
                    "total_barriers": len(all_entities["barriers"]),
                    "total_quotes": len(all_entities["quotes"]),
                    "unique_problems": len(set(all_entities["problems"])),
                    "unique_ideas": len(set(all_entities["ideas"])),
                    "unique_quotes": len(set(all_entities["quotes"]))
                },
                "ab_test_analysis": variant_stats,
                "recommendations": self._generate_recommendations(quality_scores, all_entities, variant_stats)
            }
            
            return analysis
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {e}")
            return {"error": str(e)}
    
    def _generate_recommendations(self, quality_scores: List[float], entities: Dict, variant_stats: Dict) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞"""
        recommendations = []
        
        avg_quality = np.mean(quality_scores)
        
        if avg_quality < 0.5:
            recommendations.append("üîß –ù–∏–∑–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —É–ª—É—á—à–∏—Ç—å –ø—Ä–æ–º–ø—Ç—ã –∏–ª–∏ –¥–æ–±–∞–≤–∏—Ç—å –±–æ–ª—å—à–µ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö.")
        
        if len(entities["quotes"]) < len(quality_scores) * 0.5:
            recommendations.append("üìù –ú–∞–ª–æ –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö —Ü–∏—Ç–∞—Ç. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–∞—á–µ—Å—Ç–≤–æ –¥–∏–∞–ª–æ–≥–æ–≤ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è.")
        
        if len(entities["problems"]) > len(entities["ideas"]) * 2:
            recommendations.append("‚ö†Ô∏è –ú–Ω–æ–≥–æ –ø—Ä–æ–±–ª–µ–º, –º–∞–ª–æ –∏–¥–µ–π. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —Ñ–æ–∫—É—Å –Ω–∞ –ø–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö –∞—Å–ø–µ–∫—Ç–∞—Ö.")
        
        # –ê–Ω–∞–ª–∏–∑ A/B —Ç–µ—Å—Ç–æ–≤
        if len(variant_stats) > 1:
            best_variant = max(variant_stats.items(), key=lambda x: x[1]["avg_quality"])
            recommendations.append(f"üéØ –õ—É—á—à–∏–π –≤–∞—Ä–∏–∞–Ω—Ç –ø—Ä–æ–º–ø—Ç–∞: {best_variant[0]} (–∫–∞—á–µ—Å—Ç–≤–æ: {best_variant[1]['avg_quality']:.3f})")
        
        if not recommendations:
            recommendations.append("‚úÖ –°–∏—Å—Ç–µ–º–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å—Ç–∞–±–∏–ª—å–Ω–æ. –ü—Ä–æ–¥–æ–ª–∂–∞–π—Ç–µ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥.")
        
        return recommendations
    
    def save_results(self, output_dir: str):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        with open(output_path / "final_results.json", "w", encoding="utf-8") as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        with open(output_path / "final_statistics.json", "w", encoding="utf-8") as f:
            json.dump(self.statistics, f, ensure_ascii=False, indent=2)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –æ–±—É—á–µ–Ω–∏—è
        with open(output_path / "learning_data.json", "w", encoding="utf-8") as f:
            json.dump(self.learning_data, f, ensure_ascii=False, indent=2)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ A/B —Ç–µ—Å—Ç–æ–≤
        ab_test_summary = self.adaptive_prompts.get_ab_test_summary("default_quality_test")
        with open(output_path / "ab_test_results.json", "w", encoding="utf-8") as f:
            json.dump(ab_test_summary, f, ensure_ascii=False, indent=2)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
        self._create_final_report(output_path)
        
        logger.info(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {output_dir}")
    
    def _create_final_report(self, output_path: Path):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞"""
        report = f"""
# üéØ –§–ò–ù–ê–õ–¨–ù–´–ô –û–¢–ß–ï–¢ DIALOGS RAG SYSTEM

## üìä –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
- **–í—Å–µ–≥–æ –¥–∏–∞–ª–æ–≥–æ–≤**: {self.statistics.get('total_dialogs', 0)}
- **–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –¥–∏–∞–ª–æ–≥–æ–≤**: {self.statistics.get('processed_dialogs', 0)}
- **–£—Å–ø–µ—à–Ω–æ—Å—Ç—å**: {self.statistics.get('success_rate', 0):.1%}
- **–°—Ä–µ–¥–Ω–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ**: {self.statistics.get('avg_quality_score', 0):.3f}
- **–í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏**: {self.statistics.get('processing_time_seconds', 0):.2f} —Å–µ–∫
- **–°–∫–æ—Ä–æ—Å—Ç—å**: {self.statistics.get('dialogs_per_second', 0):.2f} –¥–∏–∞–ª–æ–≥–æ–≤/—Å–µ–∫

## üß† –û–±—É—á–µ–Ω–∏–µ
- **–ü—Ä–∏–º–µ—Ä–æ–≤ –¥–æ–±–∞–≤–ª–µ–Ω–æ**: {self.statistics.get('learning_examples_added', 0)}
- **–ò—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö**: final_pipeline

## üéØ A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
- **–ê–∫—Ç–∏–≤–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤**: {len(self.statistics.get('ab_test_results', {}).get('active_tests', []))}
- **–í–∞—Ä–∏–∞–Ω—Ç–æ–≤ –ø—Ä–æ–º–ø—Ç–æ–≤**: {len(self.statistics.get('ab_test_results', {}).get('variants', []))}

## üìà –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
- **–ê–∫—Ç–∏–≤–Ω—ã—Ö –∞–ª–µ—Ä—Ç–æ–≤**: {self.statistics.get('monitoring_stats', {}).get('active_alerts', 0)}
- **–ó–¥–æ—Ä–æ–≤—å–µ —Å–∏—Å—Ç–µ–º—ã**: {self.statistics.get('monitoring_stats', {}).get('system_health', 0):.3f}

## üîß –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —Å–∏—Å—Ç–µ–º—ã
- ‚úÖ –ê–≤—Ç–æ–∫–æ—Ä—Ä–µ–∫—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞
- ‚úÖ –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã
- ‚úÖ –ù–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ
- ‚úÖ –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∫–∞—á–µ—Å—Ç–≤–∞
- ‚úÖ –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
- ‚úÖ A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ

## üìÅ –§–∞–π–ª—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
- `final_results.json` - –æ—Å–Ω–æ–≤–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
- `final_statistics.json` - —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏
- `learning_data.json` - –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
- `ab_test_results.json` - —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã A/B —Ç–µ—Å—Ç–æ–≤
- `final_report.md` - —ç—Ç–æ—Ç –æ—Ç—á–µ—Ç

---
*–û—Ç—á–µ—Ç —Å–æ–∑–¥–∞–Ω: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
        """
        
        with open(output_path / "final_report.md", "w", encoding="utf-8") as f:
            f.write(report)

def load_dialogs_from_file(file_path: str) -> List[str]:
    """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∏–∞–ª–æ–≥–æ–≤ –∏–∑ —Ñ–∞–π–ª–∞"""
    file_path = Path(file_path)
    
    if file_path.suffix == '.xlsx':
        df = pd.read_excel(file_path)
        if '–¢–µ–∫—Å—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏' in df.columns:
            dialogs = df['–¢–µ–∫—Å—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏'].dropna().tolist()
        else:
            dialogs = df.iloc[:, 0].dropna().tolist()
    elif file_path.suffix == '.json':
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        if isinstance(data, list):
            dialogs = data
        else:
            dialogs = data.get('dialogs', [])
    else:
        with open(file_path, 'r', encoding='utf-8') as f:
            dialogs = [line.strip() for line in f if line.strip()]
    
    return dialogs

async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    parser = argparse.ArgumentParser(description='–§–∏–Ω–∞–ª—å–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω Dialogs RAG System')
    parser.add_argument('--input', '-i', required=True, help='–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å –¥–∏–∞–ª–æ–≥–∞–º–∏')
    parser.add_argument('--output', '-o', default='final_results', help='–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤')
    parser.add_argument('--config', '-c', help='–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏')
    
    args = parser.parse_args()
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    if args.config and Path(args.config).exists():
        with open(args.config, 'r', encoding='utf-8') as f:
            config = json.load(f)
    else:
        config = {
            "openai_api_key": "your-api-key-here",
            "processing": {
                "enable_autocorrection": True,
                "enable_adaptive_prompts": True,
                "enable_continuous_learning": True,
                "enable_monitoring": True,
                "enable_scaling": True,
                "max_dialogs_per_batch": 1000,
                "quality_threshold": 0.6,
                "auto_save_results": True,
                "output_directory": args.output
            },
            "redis_host": "localhost",
            "redis_port": 6379,
            "redis_db": 0
        }
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∏–∞–ª–æ–≥–æ–≤
    logger.info(f"üìÇ –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∏–∞–ª–æ–≥–∏ –∏–∑ {args.input}")
    dialogs = load_dialogs_from_file(args.input)
    logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(dialogs)} –¥–∏–∞–ª–æ–≥–æ–≤")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∏ –∑–∞–ø—É—Å–∫ –ø–∞–π–ø–ª–∞–π–Ω–∞
    pipeline = FinalPipeline(config)
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∏–∞–ª–æ–≥–æ–≤
    results = await pipeline.process_dialogs(dialogs)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    pipeline.save_results(args.output)
    
    logger.info("üéâ –§–∏–Ω–∞–ª—å–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
    logger.info(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {args.output}")

if __name__ == "__main__":
    asyncio.run(main())
