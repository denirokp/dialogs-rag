#!/usr/bin/env python3
"""
Stage 3: –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–æ–∫
–ü—Ä–∏–≤–æ–¥–∏—Ç –±–∞—Ä—å–µ—Ä—ã –∏ –∏–¥–µ–∏ –∫ –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–∏–º —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞–º
"""

import json
import logging
import re
from pathlib import Path
from tqdm import tqdm
from typing import List, Dict, Any

import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent))

from config import settings
from models.validation import DialogExtraction, Citation
from prompts import NORMALIZATION_MAP

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def norm_text(s: str) -> str:
    """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏"""
    s = s.strip().lower()
    s = re.sub(r"\s+", " ", s)
    s = s.replace("—ë", "–µ")
    return s


def dedupe_variants(variants: List[str]) -> List[str]:
    """–î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ —Å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π"""
    seen = set()
    out = []
    for v in variants:
        nv = norm_text(v)
        if nv and nv not in seen:
            seen.add(nv)
            out.append(v)  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç
    return out


# –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–∏–π —Å–ª–æ–≤–∞—Ä—å —Ç–∏–ø–æ–≤ –¥–æ—Å—Ç–∞–≤–∫–∏
import json
import re

with open("canon/delivery_types_synonyms.json", "r", encoding="utf-8") as f:
    DELIVERY_CANON = json.load(f)


def canon_delivery(s: str) -> str:
    """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–∏–ø–æ–≤ –¥–æ—Å—Ç–∞–≤–∫–∏ –∫ –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–∏–º —Ñ–æ—Ä–º–∞–º"""
    t = s.strip().lower().replace("—ë", "–µ")
    for canon, syns in DELIVERY_CANON.items():
        for syn in syns + [canon]:
            ss = syn.lower().replace("—ë", "–µ")
            if t == ss:
                return canon
    return s.strip()

# –°–ª–æ–≤–∞—Ä—å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –∏–∑ prompts.py
NORMALIZE_RULES = {
    # –ë–∞—Ä—å–µ—Ä—ã
    "barriers": {
        pattern: replacement for pattern, replacement in NORMALIZATION_MAP
    },
    
    # –ò–¥–µ–∏
    "ideas": {
        r"(—Å–∫–∏–¥–∫–∞|–¥–µ—à–µ–≤–ª–µ|—Å–Ω–∏–∑–∏—Ç—å —Ü–µ–Ω—É)": "—Å–∫–∏–¥–∫–∞ –Ω–∞ –¥–æ—Å—Ç–∞–≤–∫—É",
        r"(–±–µ—Å–ø–ª–∞—Ç–Ω–∞—è –¥–æ—Å—Ç–∞–≤–∫–∞|–±–µ—Å–ø–ª–∞—Ç–Ω–æ)": "–±–µ—Å–ø–ª–∞—Ç–Ω–∞—è –¥–æ—Å—Ç–∞–≤–∫–∞",
        r"(–±—ã—Å—Ç—Ä–µ–µ|—É—Å–∫–æ—Ä–∏—Ç—å|—ç–∫—Å–ø—Ä–µ—Å—Å)": "–±—ã—Å—Ç—Ä–∞—è –¥–æ—Å—Ç–∞–≤–∫–∞",
        r"(–±–æ–ª—å—à–µ –ø–≤–∑|–¥–æ–±–∞–≤–∏—Ç—å –ø—É–Ω–∫—Ç—ã)": "–±–æ–ª—å—à–µ –ø–≤–∑",
        r"(—É–¥–æ–±–Ω–æ–µ –≤—Ä–µ–º—è|–≤—ã–±–æ—Ä –≤—Ä–µ–º–µ–Ω–∏)": "—É–¥–æ–±–Ω–æ–µ –≤—Ä–µ–º—è –¥–æ—Å—Ç–∞–≤–∫–∏",
        r"(—É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è|—Å–º—Å|–∑–≤–æ–Ω–æ–∫)": "—É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è –æ –¥–æ—Å—Ç–∞–≤–∫–µ",
        r"(–æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ|—Ç—Ä–µ–∫|—Å—Ç–∞—Ç—É—Å)": "–æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –∑–∞–∫–∞–∑–∞",
        r"(–æ–ø–ª–∞—Ç–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏|–Ω–∞–ª–æ–∂–µ–Ω–Ω—ã–π –ø–ª–∞—Ç–µ–∂)": "–æ–ø–ª–∞—Ç–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏",
        r"(–ø—Ä–∏–º–µ—Ä–∫–∞|–ø—Ä–æ–≤–µ—Ä–∫–∞|–æ—Å–º–æ—Ç—Ä)": "–ø—Ä–∏–º–µ—Ä–∫–∞ –ø–µ—Ä–µ–¥ –ø–æ–∫—É–ø–∫–æ–π",
        r"(–≤–æ–∑–≤—Ä–∞—Ç|–æ–±–º–µ–Ω|–≥–∞—Ä–∞–Ω—Ç–∏—è)": "–ª–µ–≥–∫–∏–π –≤–æ–∑–≤—Ä–∞—Ç"
    },
    
    # –°–∏–≥–Ω–∞–ª—ã
    "signals": {
        r"(–Ω–µ –ø–æ–Ω–∏–º–∞—é|–Ω–µ —Ä–∞–∑–æ–±—Ä–∞–ª—Å—è|–∑–∞–ø—É—Ç–∞–Ω–Ω–æ)": "–Ω–µ–∑–Ω–∞–Ω–∏–µ",
        r"(—Å–ª–æ–∂–Ω–æ|—Å–ª–æ–∂–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞|–∑–∞–ø—É—Ç–∞–Ω–Ω–æ)": "—Å–ª–æ–∂–Ω–æ—Å—Ç—å",
        r"(—Å—Ä–∞–≤–Ω–∏–≤–∞—é|—Å—Ä–∞–≤–Ω–µ–Ω–∏–µ|–ª—É—á—à–µ)": "—Å—Ä–∞–≤–Ω–µ–Ω–∏–µ",
        r"(—Å–æ–º–Ω–µ–≤–∞—é—Å—å|–Ω–µ —É–≤–µ—Ä–µ–Ω|–º–æ–∂–µ—Ç –±—ã—Ç—å)": "—Å–æ–º–Ω–µ–Ω–∏–µ",
        r"(–∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ|–∏–Ω—Ç–µ—Ä–µ—Å—É—é—Å—å|—É–∑–Ω–∞—Ç—å)": "–∏–Ω—Ç–µ—Ä–µ—Å",
        r"(–∂–∞–ª—É—é—Å—å|–∂–∞–ª–æ–±–∞|–Ω–µ–¥–æ–≤–æ–ª–µ–Ω)": "–∂–∞–ª–æ–±–∞",
        r"(—Ö–≤–∞–ª—é|—Ö–æ—Ä–æ—à–æ|–æ—Ç–ª–∏—á–Ω–æ)": "–ø–æ—Ö–≤–∞–ª–∞",
        r"(–ø—Ä–µ–¥–ª–∞–≥–∞—é|–∏–¥–µ—è|–º–æ–∂–Ω–æ)": "–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ"
    }
}


def normalize_text(text: str, category: str) -> str:
    """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"""
    if category not in NORMALIZE_RULES:
        return text
    
    text_lower = text.lower().strip()
    
    for pattern, replacement in NORMALIZE_RULES[category].items():
        if re.search(pattern, text_lower):
            return replacement
    
    return text


def normalize_phrase(text: str) -> str:
    """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ñ—Ä–∞–∑—ã —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º NORMALIZATION_MAP"""
    t = text.strip().lower()
    for rx, repl in NORMALIZATION_MAP:
        if re.search(rx, t, re.IGNORECASE):
            t = repl
            break
    return t


def normalize_dialog(dialog: DialogExtraction) -> DialogExtraction:
    """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∏–∞–ª–æ–≥–∞"""
    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –±–∞—Ä—å–µ—Ä–æ–≤
    normalized_barriers = []
    for barrier in dialog.barriers:
        normalized = normalize_text(barrier, "barriers")
        if normalized not in normalized_barriers:
            normalized_barriers.append(normalized)
    
    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏–¥–µ–π
    normalized_ideas = []
    for idea in dialog.ideas:
        normalized = normalize_text(idea, "ideas")
        if normalized not in normalized_ideas:
            normalized_ideas.append(normalized)
    
    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å–∏–≥–Ω–∞–ª–æ–≤
    normalized_signals = []
    for signal in dialog.signals:
        normalized = normalize_text(signal, "signals")
        if normalized not in normalized_signals:
            normalized_signals.append(normalized)
    
    # –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤
    normalized_barriers = dedupe_variants(normalized_barriers)
    normalized_ideas = dedupe_variants(normalized_ideas)
    normalized_signals = dedupe_variants(normalized_signals)
    
    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–∏–ø–æ–≤ –¥–æ—Å—Ç–∞–≤–∫–∏
    normalized_delivery_types = [canon_delivery(dt) for dt in dialog.delivery_types]
    
    # Per-dialog –ø–æ–¥—Å—á–µ—Ç—ã (—É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Ç–∏–ø—ã)
    types = set(canon_delivery(x) for x in dialog.delivery_types)
    # –ù–µ –¥–æ–±–∞–≤–ª—è–µ–º –≤ –æ–±—ä–µ–∫—Ç, —Ç–∞–∫ –∫–∞–∫ —ç—Ç–æ –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ –≤ –º–æ–¥–µ–ª–∏
    normalized_delivery_types = list(set(normalized_delivery_types))  # —É–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ –¥–∏–∞–ª–æ–≥–∞
    return DialogExtraction(
        dialog_id=dialog.dialog_id,
        delivery_discussed=dialog.delivery_discussed,
        delivery_types=normalized_delivery_types,
        barriers=normalized_barriers,
        ideas=normalized_ideas,
        signals=normalized_signals,
        citations=dialog.citations,
        region=dialog.region,
        segment=dialog.segment,
        product_category=dialog.product_category,
        sentiment=dialog.sentiment,
        extras=dialog.extras
    )


def load_extracted_dialogs() -> List[DialogExtraction]:
    """–ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö –¥–∏–∞–ª–æ–≥–æ–≤"""
    extracted_file = Path("artifacts/stage2_extracted.jsonl")
    
    if not extracted_file.exists():
        logger.error(f"‚ùå –§–∞–π–ª {extracted_file} –Ω–µ –Ω–∞–π–¥–µ–Ω. –ó–∞–ø—É—Å—Ç–∏—Ç–µ Stage 2 —Å–Ω–∞—á–∞–ª–∞.")
        return []
    
    dialogs = []
    
    with open(extracted_file, 'r', encoding='utf-8') as f:
        for line in f:
            try:
                data = json.loads(line.strip())
                dialog = DialogExtraction(**data)
                dialogs.append(dialog)
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –¥–∏–∞–ª–æ–≥–∞: {e}")
                continue
    
    return dialogs


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è Stage 3"""
    logger.info("üöÄ Stage 3: –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–æ–∫")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–ø–∫–∏ artifacts
    artifacts_dir = Path("artifacts")
    artifacts_dir.mkdir(exist_ok=True)
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö –¥–∏–∞–ª–æ–≥–æ–≤
    dialogs = load_extracted_dialogs()
    logger.info(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(dialogs)} –¥–∏–∞–ª–æ–≥–æ–≤ –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏")
    
    if not dialogs:
        logger.error("‚ùå –ù–µ—Ç –¥–∏–∞–ª–æ–≥–æ–≤ –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏")
        return
    
    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
    logger.info("üîÑ –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–æ–∫...")
    normalized_dialogs = []
    
    for dialog in tqdm(dialogs, desc="–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è"):
        try:
            normalized_dialog = normalize_dialog(dialog)
            normalized_dialogs.append(normalized_dialog)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–∏–∞–ª–æ–≥–∞ {dialog.dialog_id}: {e}")
            continue
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    output_file = artifacts_dir / "stage3_normalized.jsonl"
    logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ {output_file}")
    
    with open(output_file, 'w', encoding='utf-8') as f:
        for dialog in normalized_dialogs:
            f.write(json.dumps(dialog.dict()) + '\n')
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
    original_barriers = set()
    original_ideas = set()
    original_signals = set()
    
    normalized_barriers = set()
    normalized_ideas = set()
    normalized_signals = set()
    
    for dialog in dialogs:
        original_barriers.update(dialog.barriers)
        original_ideas.update(dialog.ideas)
        original_signals.update(dialog.signals)
    
    for dialog in normalized_dialogs:
        normalized_barriers.update(dialog.barriers)
        normalized_ideas.update(dialog.ideas)
        normalized_signals.update(dialog.signals)
    
    logger.info("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏:")
    logger.info(f"  –ë–∞—Ä—å–µ—Ä—ã: {len(original_barriers)} ‚Üí {len(normalized_barriers)}")
    logger.info(f"  –ò–¥–µ–∏: {len(original_ideas)} ‚Üí {len(normalized_ideas)}")
    logger.info(f"  –°–∏–≥–Ω–∞–ª—ã: {len(original_signals)} ‚Üí {len(normalized_signals)}")
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
    if original_barriers:
        logger.info("üîç –ü—Ä–∏–º–µ—Ä—ã –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –±–∞—Ä—å–µ—Ä–æ–≤:")
        for original in list(original_barriers)[:5]:
            normalized = normalize_text(original, "barriers")
            if original != normalized:
                logger.info(f"  '{original}' ‚Üí '{normalized}'")
    
    logger.info("‚úÖ Stage 3 –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")


if __name__ == "__main__":
    main()
