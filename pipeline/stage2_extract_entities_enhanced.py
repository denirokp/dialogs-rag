#!/usr/bin/env python3
"""
Stage 2 Enhanced: –£–ª—É—á—à–µ–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π
–ò–∑–≤–ª–µ–∫–∞–µ—Ç –±–∞—Ä—å–µ—Ä—ã, –∏–¥–µ–∏, —Å–∏–≥–Ω–∞–ª—ã —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–º –∞–Ω–∞–ª–∏–∑–æ–º
"""

import json
import logging
import pandas as pd
import re
from pathlib import Path
from tqdm import tqdm
from tenacity import retry, stop_after_attempt, wait_exponential
from typing import List, Dict, Any, Set

import sys
sys.path.append(str(Path(__file__).parent.parent))

import openai
from config import settings
from models.validation import DeliveryDetection, DialogExtraction, Citation
from prompts import STAGE_CONFIG, DELIVERY_KEYWORDS

# –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —Ç–∏–ø—ã —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
ENHANCED_SENTIMENTS = {
    "—Ä–∞–∑–¥—Ä–∞–∂–µ–Ω–∏–µ": ["–ª–µ–≥–∫–æ–µ_–Ω–µ–¥–æ–≤–æ–ª—å—Å—Ç–≤–æ", "—Ñ—Ä—É—Å—Ç—Ä–∞—Ü–∏—è", "–≥–Ω–µ–≤", "—è—Ä–æ—Å—Ç—å"],
    "–ø–æ–∑–∏—Ç–∏–≤": ["—É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–µ–Ω–∏–µ", "—ç–Ω—Ç—É–∑–∏–∞–∑–º", "–≤–æ—Å—Ç–æ—Ä–≥", "–±–ª–∞–≥–æ–¥–∞—Ä–Ω–æ—Å—Ç—å"],
    "—Å–æ–º–Ω–µ–Ω–∏–µ": ["–Ω–µ—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å", "—Ç—Ä–µ–≤–æ–≥–∞", "–ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å", "—Å–∫–µ–ø—Ç–∏—Ü–∏–∑–º"],
    "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ": ["–±–µ–∑—Ä–∞–∑–ª–∏—á–∏–µ", "—Å–ø–æ–∫–æ–π—Å—Ç–≤–∏–µ", "–¥–µ–ª–æ–≤–∏—Ç–æ—Å—Ç—å", "—Ñ–æ—Ä–º–∞–ª—å–Ω–æ—Å—Ç—å"]
}

# –£—Ä–æ–≤–Ω–∏ —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ—Å—Ç–∏
EXPERTISE_LEVELS = ["–Ω–æ–≤–∏—á–æ–∫", "—Å—Ä–µ–¥–Ω–∏–π", "—ç–∫—Å–ø–µ—Ä—Ç"]

# –£—Ä–æ–≤–Ω–∏ —Å—Ä–æ—á–Ω–æ—Å—Ç–∏
URGENCY_LEVELS = ["–∫—Ä–∏—Ç–∏—á–Ω–æ", "–≤–∞–∂–Ω–æ", "–Ω–µ–≤–∞–∂–Ω–æ"]

# –í–ª–∏—è–Ω–∏–µ –Ω–∞ —Ä–µ—à–µ–Ω–∏–µ
DECISION_IMPACT = ["–±–ª–æ–∫–∏—Ä—É–µ—Ç –ø–æ–∫—É–ø–∫—É", "–≤–ª–∏—è–µ—Ç –Ω–∞ –≤—ã–±–æ—Ä", "–Ω–µ –≤–ª–∏—è–µ—Ç"]

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def load_enhanced_prompt() -> str:
    """–ó–∞–≥—Ä—É–∑–∫–∞ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞"""
    prompt_file = Path("prompts/extract_entities_enhanced.txt")
    if prompt_file.exists():
        return prompt_file.read_text(encoding="utf-8")
    else:
        # Fallback –∫ –±–∞–∑–æ–≤–æ–º—É –ø—Ä–æ–º–ø—Ç—É
        return load_basic_prompt()

def load_basic_prompt() -> str:
    """–ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑–æ–≤–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞"""
    prompt_file = Path("prompts/extract_entities.txt")
    return prompt_file.read_text(encoding="utf-8")

def normalize_enhanced_sentiment(sentiment: str) -> str:
    """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏"""
    if not sentiment:
        return "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ"
    
    sentiment = sentiment.strip().lower()
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –±–∞–∑–æ–≤—ã–µ —Ç–∏–ø—ã
    for base_sentiment, variants in ENHANCED_SENTIMENTS.items():
        if sentiment == base_sentiment or sentiment in variants:
            return base_sentiment
    
    return "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ"

def validate_enhanced_extraction(extraction: Dict[str, Any]) -> Dict[str, Any]:
    """–í–∞–ª–∏–¥–∞—Ü–∏—è —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è"""
    errors = []
    
    # –í–∞–ª–∏–¥–∞—Ü–∏—è –±–∞—Ä—å–µ—Ä–æ–≤
    if "barriers" in extraction:
        for barrier in extraction["barriers"]:
            if isinstance(barrier, dict):
                required_fields = ["category", "text", "severity", "context"]
                for field in required_fields:
                    if field not in barrier:
                        errors.append(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –ø–æ–ª–µ {field} –≤ –±–∞—Ä—å–µ—Ä–µ")
            elif isinstance(barrier, str):
                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Å—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç –≤ –Ω–æ–≤—ã–π
                extraction["barriers"] = [{
                    "category": "–æ–±—â–∏–µ",
                    "text": barrier,
                    "severity": "—Å—Ä–µ–¥–Ω—è—è",
                    "context": "–≤—ã—è–≤–ª–µ–Ω–æ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏"
                }]
    
    # –í–∞–ª–∏–¥–∞—Ü–∏—è –∏–¥–µ–π
    if "ideas" in extraction:
        for idea in extraction["ideas"]:
            if isinstance(idea, dict):
                required_fields = ["category", "text", "feasibility", "impact"]
                for field in required_fields:
                    if field not in idea:
                        errors.append(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –ø–æ–ª–µ {field} –≤ –∏–¥–µ–µ")
            elif isinstance(idea, str):
                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Å—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç –≤ –Ω–æ–≤—ã–π
                extraction["ideas"] = [{
                    "category": "–æ–±—â–∏–µ",
                    "text": idea,
                    "feasibility": "—Å—Ä–µ–¥–Ω—è—è",
                    "impact": "—Å—Ä–µ–¥–Ω–∏–π"
                }]
    
    # –í–∞–ª–∏–¥–∞—Ü–∏—è —Å–∏–≥–Ω–∞–ª–æ–≤
    if "signals" in extraction:
        for signal in extraction["signals"]:
            if isinstance(signal, dict):
                required_fields = ["type", "text", "confidence", "context"]
                for field in required_fields:
                    if field not in signal:
                        errors.append(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –ø–æ–ª–µ {field} –≤ —Å–∏–≥–Ω–∞–ª–µ")
            elif isinstance(signal, str):
                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Å—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç –≤ –Ω–æ–≤—ã–π
                extraction["signals"] = [{
                    "type": "–ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è",
                    "text": signal,
                    "confidence": "—Å—Ä–µ–¥–Ω—è—è",
                    "context": "–≤—ã—è–≤–ª–µ–Ω–æ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏"
                }]
    
    # –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã—Ö –ø–æ–ª–µ–π
    if "emotional_state" in extraction:
        extraction["emotional_state"] = normalize_enhanced_sentiment(extraction["emotional_state"])
    
    if "expertise_level" in extraction and extraction["expertise_level"] not in EXPERTISE_LEVELS:
        extraction["expertise_level"] = "—Å—Ä–µ–¥–Ω–∏–π"
    
    if "urgency" in extraction and extraction["urgency"] not in URGENCY_LEVELS:
        extraction["urgency"] = "–≤–∞–∂–Ω–æ"
    
    if "decision_impact" in extraction and extraction["decision_impact"] not in DECISION_IMPACT:
        extraction["decision_impact"] = "–≤–ª–∏—è–µ—Ç –Ω–∞ –≤—ã–±–æ—Ä"
    
    if errors:
        logger.warning(f"–û—à–∏–±–∫–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {errors}")
    
    return extraction

@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
def extract_entities_enhanced(dialog_text: str, dialog_id: str) -> Dict[str, Any]:
    """–£–ª—É—á—à–µ–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π –∏–∑ –¥–∏–∞–ª–æ–≥–∞"""
    
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç
        prompt = load_enhanced_prompt()
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å
        messages = [
            {"role": "system", "content": prompt},
            {"role": "user", "content": f"–î–∏–∞–ª–æ–≥ ID: {dialog_id}\n\n{dialog_text}"}
        ]
        
        # –í—ã–∑—ã–≤–∞–µ–º API
        from openai import OpenAI
        client = OpenAI(api_key=settings.openai_api_key)
        
        response = client.chat.completions.create(
            model=settings.model_extract,
            messages=messages,
            temperature=0.1,
            max_tokens=2000
        )
        
        # –ü–∞—Ä—Å–∏–º –æ—Ç–≤–µ—Ç
        content = response.choices[0].message.content.strip()
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º JSON
        json_match = re.search(r'\{.*\}', content, re.DOTALL)
        if not json_match:
            raise ValueError("JSON –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –æ—Ç–≤–µ—Ç–µ")
        
        extraction = json.loads(json_match.group())
        
        # –í–∞–ª–∏–¥–∏—Ä—É–µ–º –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º
        extraction = validate_enhanced_extraction(extraction)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        extraction["dialog_id"] = dialog_id
        extraction["extraction_timestamp"] = pd.Timestamp.now().isoformat()
        extraction["model_used"] = settings.model_extract
        
        return extraction
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–ª—è –¥–∏–∞–ª–æ–≥–∞ {dialog_id}: {e}")
        return {
            "dialog_id": dialog_id,
            "delivery_related": False,
            "barriers": [],
            "ideas": [],
            "signals": [],
            "delivery_types": [],
            "product_category": "",
            "sentiment": "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ",
            "emotional_state": "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ",
            "expertise_level": "—Å—Ä–µ–¥–Ω–∏–π",
            "urgency": "–≤–∞–∂–Ω–æ",
            "decision_impact": "–Ω–µ –≤–ª–∏—è–µ—Ç",
            "citations": [],
            "error": str(e)
        }

def process_dialogs_enhanced(input_file: str, output_file: str):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∏–∞–ª–æ–≥–æ–≤ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ–º"""
    
    logger.info("üöÄ Stage 2 Enhanced: –£–ª—É—á—à–µ–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã stage1
    stage1_file = "artifacts/stage1_delivery.jsonl"
    if not Path(stage1_file).exists():
        logger.error(f"–§–∞–π–ª {stage1_file} –Ω–µ –Ω–∞–π–¥–µ–Ω. –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ Stage 1.")
        return
    
    # –ß–∏—Ç–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã stage1
    delivery_dialogs = []
    with open(stage1_file, 'r', encoding='utf-8') as f:
        for line in f:
            data = json.loads(line.strip())
            if data.get('delivery_discussed', False):
                delivery_dialogs.append(data)
    
    logger.info(f"–î–∏–∞–ª–æ–≥–æ–≤ —Å –¥–æ—Å—Ç–∞–≤–∫–æ–π: {len(delivery_dialogs)}")
    
    if not delivery_dialogs:
        logger.warning("–ù–µ—Ç –¥–∏–∞–ª–æ–≥–æ–≤ —Å –¥–æ—Å—Ç–∞–≤–∫–æ–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
        return
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∏–∞–ª–æ–≥–∏ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞
    original_df = pd.read_excel("data/dialogs.xlsx", engine='openpyxl')
    
    results = []
    
    for dialog_data in tqdm(delivery_dialogs, desc="–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π"):
        dialog_id = dialog_data.get('dialog_id', 'unknown')
        
        # –ù–∞—Ö–æ–¥–∏–º –∏—Å—Ö–æ–¥–Ω—ã–π –¥–∏–∞–ª–æ–≥ –ø–æ ID
        original_dialog = original_df[original_df['ID –∑–≤–æ–Ω–∫–∞'].astype(str) == dialog_id]
        if original_dialog.empty:
            logger.warning(f"–î–∏–∞–ª–æ–≥ {dialog_id} –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
            continue
            
        dialog_text = str(original_dialog.iloc[0]['–¢–µ–∫—Å—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏'])
        
        if not dialog_text or len(dialog_text.strip()) < 10:
            logger.warning(f"–ü—Ä–æ–ø—É—Å–∫ –¥–∏–∞–ª–æ–≥–∞ {dialog_id}: –ø—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç")
            continue
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—É—â–Ω–æ—Å—Ç–∏
        extraction = extract_entities_enhanced(dialog_text, dialog_id)
        results.append(extraction)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    output_path = Path(output_file)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        for result in results:
            f.write(json.dumps(result, ensure_ascii=False) + '\n')
    
    logger.info(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ {output_file}")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    total_barriers = sum(len(r.get('barriers', [])) for r in results)
    total_ideas = sum(len(r.get('ideas', [])) for r in results)
    total_signals = sum(len(r.get('signals', [])) for r in results)
    
    logger.info(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    logger.info(f"  –ë–∞—Ä—å–µ—Ä–æ–≤: {total_barriers}")
    logger.info(f"  –ò–¥–µ–π: {total_ideas}")
    logger.info(f"  –°–∏–≥–Ω–∞–ª–æ–≤: {total_signals}")
    
    return results

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    input_file = "artifacts/stage1_delivery.jsonl"
    output_file = "artifacts/stage2_extracted_enhanced.jsonl"
    
    process_dialogs_enhanced(input_file, output_file)

if __name__ == "__main__":
    main()
