#!/usr/bin/env python3
"""
Stage 4 Enhanced: –£–ª—É—á—à–µ–Ω–Ω–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º –∞–Ω–∞–ª–∏–∑–æ–º
–ì—Ä—É–ø–ø–∏—Ä—É–µ—Ç –ø–æ—Ö–æ–∂–∏–µ –±–∞—Ä—å–µ—Ä—ã –∏ –∏–¥–µ–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º embeddings
"""

import json
import logging
import re
import pandas as pd
from pathlib import Path
from collections import Counter, defaultdict
from typing import List, Dict, Any, Set, Tuple
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans, DBSCAN
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics import silhouette_score

import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent))

from config import settings
from models.validation import DialogExtraction, Cluster, ClusterVariant, ClusterSlices, ClustersData
from prompts import STAGE_CONFIG, CLUSTERING_STOPWORDS

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def semantic_cluster(texts: List[str], threshold: float = 0.7, min_samples: int = 2) -> Dict[int, List[str]]:
    """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º TF-IDF –∏ DBSCAN"""
    
    if len(texts) < 2:
        return {0: texts}
    
    # –°–æ–∑–¥–∞–µ–º TF-IDF –≤–µ–∫—Ç–æ—Ä—ã
    vectorizer = TfidfVectorizer(
        max_features=1000,
        stop_words=list(CLUSTERING_STOPWORDS),
        ngram_range=(1, 2),
        min_df=1
    )
    
    try:
        tfidf_matrix = vectorizer.fit_transform(texts)
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º DBSCAN –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
        clustering = DBSCAN(
            eps=1-threshold,
            min_samples=min_samples,
            metric='cosine'
        )
        
        cluster_labels = clustering.fit_predict(tfidf_matrix)
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º
        clusters = {}
        for i, label in enumerate(cluster_labels):
            if label not in clusters:
                clusters[label] = []
            clusters[label].append(texts[i])
        
        return clusters
        
    except Exception as e:
        logger.warning(f"–û—à–∏–±–∫–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏: {e}")
        # Fallback –∫ –ø—Ä–æ—Å—Ç–æ–π –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–µ
        return {0: texts}

def merge_semantic_clusters(clusters: Dict[int, List[str]]) -> List[Dict[str, Any]]:
    """–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –ø–æ—Ö–æ–∂–∏—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤"""
    
    merged = []
    
    for cluster_id, texts in clusters.items():
        if cluster_id == -1:  # –®—É–º –≤ DBSCAN
            continue
            
        if not texts:
            continue
        
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —á–∞—Å—Ç–æ—Ç—É
        text_counts = Counter(texts)
        
        # –°–æ–∑–¥–∞–µ–º –≤–∞—Ä–∏–∞–Ω—Ç—ã
        variants = []
        for text, count in text_counts.most_common():
            variants.append({
                "text": text,
                "count": count
            })
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–∞
        cluster_name = generate_cluster_name(texts)
        
        # –°–æ–∑–¥–∞–µ–º –∫–ª–∞—Å—Ç–µ—Ä
        cluster = {
            "name": cluster_name,
            "variants": variants,
            "dialog_ids": [],  # –ë—É–¥–µ—Ç –∑–∞–ø–æ–ª–Ω–µ–Ω–æ –ø–æ–∑–∂–µ
            "slices": {
                "regions": {},
                "segments": {},
                "product_categories": {},
                "delivery_types": {},
                "sentiment": {}
            },
            "semantic_coherence": calculate_semantic_coherence(texts),
            "priority": calculate_cluster_priority(texts),
            "description": generate_cluster_description(texts)
        }
        
        merged.append(cluster)
    
    return merged

def generate_cluster_name(texts: List[str]) -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–∞–∑–≤–∞–Ω–∏—è –∫–ª–∞—Å—Ç–µ—Ä–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–æ–≤"""
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
    words = []
    for text in texts:
        # –ü—Ä–æ—Å—Ç–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
        text_words = re.findall(r'\b[–∞-—è—ë]+\b', text.lower())
        words.extend([w for w in text_words if len(w) > 2 and w not in CLUSTERING_STOPWORDS])
    
    # –ù–∞—Ö–æ–¥–∏–º –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç—ã–µ —Å–ª–æ–≤–∞
    word_counts = Counter(words)
    top_words = [word for word, count in word_counts.most_common(3)]
    
    if top_words:
        return " ".join(top_words)
    else:
        return "–ù–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π –∫–ª–∞—Å—Ç–µ—Ä"

def calculate_semantic_coherence(texts: List[str]) -> float:
    """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å–≤—è–∑–Ω–æ—Å—Ç–∏ –∫–ª–∞—Å—Ç–µ—Ä–∞"""
    
    if len(texts) < 2:
        return 1.0
    
    try:
        # –°–æ–∑–¥–∞–µ–º TF-IDF –≤–µ–∫—Ç–æ—Ä—ã
        vectorizer = TfidfVectorizer(
            max_features=500,
            stop_words=list(CLUSTERING_STOPWORDS),
            ngram_range=(1, 2)
        )
        
        tfidf_matrix = vectorizer.fit_transform(texts)
        
        # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
        similarity_matrix = cosine_similarity(tfidf_matrix)
        
        # –£–±–∏—Ä–∞–µ–º –¥–∏–∞–≥–æ–Ω–∞–ª—å (—Å—Ö–æ–¥—Å—Ç–≤–æ —Å —Å–∞–º–∏–º —Å–æ–±–æ–π)
        mask = np.ones(similarity_matrix.shape, dtype=bool)
        np.fill_diagonal(mask, False)
        
        # –°—Ä–µ–¥–Ω–µ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
        mean_similarity = similarity_matrix[mask].mean()
        
        return float(mean_similarity)
        
    except Exception as e:
        logger.warning(f"–û—à–∏–±–∫–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Å–≤—è–∑–Ω–æ—Å—Ç–∏: {e}")
        return 0.5

def calculate_cluster_priority(texts: List[str]) -> str:
    """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞ –∫–ª–∞—Å—Ç–µ—Ä–∞"""
    
    # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞
    high_priority_keywords = [
        "–∫—Ä–∏—Ç–∏—á–Ω–æ", "—Å—Ä–æ—á–Ω–æ", "–±–ª–æ–∫–∏—Ä—É–µ—Ç", "–Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç", "–æ—à–∏–±–∫–∞",
        "–Ω–µ –ø–æ–Ω–∏–º–∞—é", "–∫–∞–∫ —Å–¥–µ–ª–∞—Ç—å", "–ø–æ–º–æ–≥–∏—Ç–µ"
    ]
    
    medium_priority_keywords = [
        "–ø—Ä–æ–±–ª–µ–º–∞", "—Å–ª–æ–∂–Ω–æ", "–Ω–µ—É–¥–æ–±–Ω–æ", "–¥–æ–ª–≥–æ", "–¥–æ—Ä–æ–≥–æ"
    ]
    
    texts_lower = [text.lower() for text in texts]
    
    for keyword in high_priority_keywords:
        if any(keyword in text for text in texts_lower):
            return "–≤—ã—Å–æ–∫–∏–π"
    
    for keyword in medium_priority_keywords:
        if any(keyword in text for text in texts_lower):
            return "—Å—Ä–µ–¥–Ω–∏–π"
    
    return "–Ω–∏–∑–∫–∏–π"

def generate_cluster_description(texts: List[str]) -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ–ø–∏—Å–∞–Ω–∏—è –∫–ª–∞—Å—Ç–µ—Ä–∞"""
    
    if not texts:
        return "–ü—É—Å—Ç–æ–π –∫–ª–∞—Å—Ç–µ—Ä"
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –æ–±—â–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
    common_words = []
    for text in texts:
        words = re.findall(r'\b[–∞-—è—ë]+\b', text.lower())
        common_words.extend([w for w in words if len(w) > 2])
    
    word_counts = Counter(common_words)
    top_words = [word for word, count in word_counts.most_common(5)]
    
    if top_words:
        return f"–ö–ª–∞—Å—Ç–µ—Ä —Å–≤—è–∑–∞–Ω —Å: {', '.join(top_words)}"
    else:
        return "–û–ø–∏—Å–∞–Ω–∏–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ"

def enhanced_clustering(extractions: List[Dict[str, Any]]) -> Dict[str, Any]:
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º –∞–Ω–∞–ª–∏–∑–æ–º"""
    
    logger.info("üîç –ó–∞–ø—É—Å–∫ —É–ª—É—á—à–µ–Ω–Ω–æ–π –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏...")
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç—ã –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
    barriers = []
    ideas = []
    signals = []
    
    for extraction in extractions:
        # –ë–∞—Ä—å–µ—Ä—ã
        for barrier in extraction.get("barriers", []):
            if isinstance(barrier, dict):
                barriers.append(barrier["text"])
            else:
                barriers.append(barrier)
        
        # –ò–¥–µ–∏
        for idea in extraction.get("ideas", []):
            if isinstance(idea, dict):
                ideas.append(idea["text"])
            else:
                ideas.append(idea)
        
        # –°–∏–≥–Ω–∞–ª—ã
        for signal in extraction.get("signals", []):
            if isinstance(signal, dict):
                signals.append(signal["text"])
            else:
                signals.append(signal)
    
    logger.info(f"–ù–∞–π–¥–µ–Ω–æ: {len(barriers)} –±–∞—Ä—å–µ—Ä–æ–≤, {len(ideas)} –∏–¥–µ–π, {len(signals)} —Å–∏–≥–Ω–∞–ª–æ–≤")
    
    # –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
    barrier_clusters = semantic_cluster(barriers, threshold=0.6)
    idea_clusters = semantic_cluster(ideas, threshold=0.7)
    signal_clusters = semantic_cluster(signals, threshold=0.8)
    
    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ
    merged_barriers = merge_semantic_clusters(barrier_clusters)
    merged_ideas = merge_semantic_clusters(idea_clusters)
    merged_signals = merge_semantic_clusters(signal_clusters)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    for cluster in merged_barriers + merged_ideas + merged_signals:
        cluster["cluster_type"] = "barrier" if cluster in merged_barriers else "idea" if cluster in merged_ideas else "signal"
        cluster["created_at"] = pd.Timestamp.now().isoformat()
    
    result = {
        "barriers": merged_barriers,
        "ideas": merged_ideas,
        "signals": merged_signals,
        "clustering_metrics": {
            "barrier_clusters": len(merged_barriers),
            "idea_clusters": len(merged_ideas),
            "signal_clusters": len(merged_signals),
            "total_clusters": len(merged_barriers) + len(merged_ideas) + len(merged_signals)
        }
    }
    
    logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {result['clustering_metrics']['total_clusters']}")
    
    return result

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    logger.info("üöÄ Stage 4 Enhanced: –£–ª—É—á—à–µ–Ω–Ω–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    input_file = "artifacts/stage2_extracted_enhanced.jsonl"
    if not Path(input_file).exists():
        logger.error(f"–§–∞–π–ª {input_file} –Ω–µ –Ω–∞–π–¥–µ–Ω. –ó–∞–ø—É—Å—Ç–∏—Ç–µ Stage 2 Enhanced —Å–Ω–∞—á–∞–ª–∞.")
        return
    
    extractions = []
    with open(input_file, 'r', encoding='utf-8') as f:
        for line in f:
            extractions.append(json.loads(line))
    
    logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(extractions)} –∏–∑–≤–ª–µ—á–µ–Ω–∏–π")
    
    # –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
    clusters = enhanced_clustering(extractions)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    output_file = "artifacts/stage4_clusters_enhanced.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(clusters, f, ensure_ascii=False, indent=2)
    
    logger.info(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {output_file}")

if __name__ == "__main__":
    main()
