#!/usr/bin/env python3
"""
Stage 4.5: –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ –æ–±–æ–≥–∞—â–µ–Ω–∏–µ
–û–±–æ–≥–∞—â–µ–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
"""

import json
import logging
from pathlib import Path
from typing import List, Dict, Any
import pandas as pd
import re
from collections import Counter

import sys
sys.path.append(str(Path(__file__).parent.parent))

from config import settings

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def generate_cluster_description(cluster: Dict[str, Any]) -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ–ø–∏—Å–∞–Ω–∏—è –∫–ª–∞—Å—Ç–µ—Ä–∞"""
    
    variants = cluster.get("variants", [])
    if not variants:
        return "–ü—É—Å—Ç–æ–π –∫–ª–∞—Å—Ç–µ—Ä"
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≤–∞—Ä–∏–∞–Ω—Ç—ã
    texts = [v["text"] for v in variants]
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
    words = []
    for text in texts:
        text_words = re.findall(r'\b[–∞-—è—ë]+\b', text.lower())
        words.extend([w for w in text_words if len(w) > 2])
    
    word_counts = Counter(words)
    top_words = [word for word, count in word_counts.most_common(5)]
    
    if top_words:
        return f"–ö–ª–∞—Å—Ç–µ—Ä —Å–≤—è–∑–∞–Ω —Å: {', '.join(top_words)}"
    else:
        return "–û–ø–∏—Å–∞–Ω–∏–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ"

def calculate_cluster_priority(cluster: Dict[str, Any]) -> str:
    """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞ –∫–ª–∞—Å—Ç–µ—Ä–∞"""
    
    variants = cluster.get("variants", [])
    if not variants:
        return "–Ω–∏–∑–∫–∏–π"
    
    # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞
    high_priority_keywords = [
        "–∫—Ä–∏—Ç–∏—á–Ω–æ", "—Å—Ä–æ—á–Ω–æ", "–±–ª–æ–∫–∏—Ä—É–µ—Ç", "–Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç", "–æ—à–∏–±–∫–∞",
        "–Ω–µ –ø–æ–Ω–∏–º–∞—é", "–∫–∞–∫ —Å–¥–µ–ª–∞—Ç—å", "–ø–æ–º–æ–≥–∏—Ç–µ", "–≤—ã—Å–æ–∫–∞—è"
    ]
    
    medium_priority_keywords = [
        "–ø—Ä–æ–±–ª–µ–º–∞", "—Å–ª–æ–∂–Ω–æ", "–Ω–µ—É–¥–æ–±–Ω–æ", "–¥–æ–ª–≥–æ", "–¥–æ—Ä–æ–≥–æ", "—Å—Ä–µ–¥–Ω—è—è"
    ]
    
    texts = [v["text"].lower() for v in variants]
    
    for keyword in high_priority_keywords:
        if any(keyword in text for text in texts):
            return "–≤—ã—Å–æ–∫–∏–π"
    
    for keyword in medium_priority_keywords:
        if any(keyword in text for text in texts):
            return "—Å—Ä–µ–¥–Ω–∏–π"
    
    return "–Ω–∏–∑–∫–∏–π"

def generate_solution_suggestions(cluster: Dict[str, Any]) -> List[str]:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –ø–æ —Ä–µ—à–µ–Ω–∏—é"""
    
    cluster_name = cluster.get("name", "").lower()
    suggestions = []
    
    # –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–∑–≤–∞–Ω–∏—è –∫–ª–∞—Å—Ç–µ—Ä–∞
    if "–Ω–µ–ø–æ–Ω–∏–º–∞–Ω–∏–µ" in cluster_name or "–ø—Ä–æ—Ü–µ—Å—Å" in cluster_name:
        suggestions.extend([
            "–°–æ–∑–¥–∞—Ç—å –ø–æ—à–∞–≥–æ–≤—É—é –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—é",
            "–î–æ–±–∞–≤–∏—Ç—å –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–µ –ø–æ–¥—Å–∫–∞–∑–∫–∏",
            "–£–ª—É—á—à–∏—Ç—å UX –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞"
        ])
    
    if "–Ω–∞—Å—Ç—Ä–æ–π–∫–∏" in cluster_name or "—Å–±–æ–∏" in cluster_name:
        suggestions.extend([
            "–£–ø—Ä–æ—Å—Ç–∏—Ç—å –ø—Ä–æ—Ü–µ—Å—Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∏",
            "–î–æ–±–∞–≤–∏—Ç—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫—É—é –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫—É",
            "–£–ª—É—á—à–∏—Ç—å —Ç–µ—Ö–Ω–∏—á–µ—Å–∫—É—é –ø–æ–¥–¥–µ—Ä–∂–∫—É"
        ])
    
    if "—Å—Ç–æ–∏–º–æ—Å—Ç—å" in cluster_name or "–¥–æ—Ä–æ–≥–æ" in cluster_name:
        suggestions.extend([
            "–ü—Ä–µ–¥–æ—Å—Ç–∞–≤–∏—Ç—å –∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä —Å—Ç–æ–∏–º–æ—Å—Ç–∏",
            "–î–æ–±–∞–≤–∏—Ç—å –≤–∞—Ä–∏–∞–Ω—Ç—ã —ç–∫–æ–Ω–æ–º–∏—á–Ω–æ–π –¥–æ—Å—Ç–∞–≤–∫–∏",
            "–í–Ω–µ–¥—Ä–∏—Ç—å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ —Ü–µ–Ω–æ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ"
        ])
    
    if "–ø–≤–∑" in cluster_name or "–ø—É–Ω–∫—Ç—ã" in cluster_name:
        suggestions.extend([
            "–†–∞—Å—à–∏—Ä–∏—Ç—å —Å–µ—Ç—å –ø—É–Ω–∫—Ç–æ–≤ –≤—ã–¥–∞—á–∏",
            "–£–ª—É—á—à–∏—Ç—å –∫–∞—Ä—Ç—É —Å –ü–í–ó",
            "–î–æ–±–∞–≤–∏—Ç—å —Ñ–∏–ª—å—Ç—Ä—ã –ø–æ —É–¥–æ–±—Å—Ç–≤—É"
        ])
    
    # –ï—Å–ª–∏ –Ω–µ—Ç —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, –¥–æ–±–∞–≤–ª—è–µ–º –æ–±—â–∏–µ
    if not suggestions:
        suggestions = [
            "–ü—Ä–æ–≤–µ—Å—Ç–∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ",
            "–°–æ–±—Ä–∞—Ç—å –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π",
            "–†–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞"
        ]
    
    return suggestions[:3]  # –ú–∞–∫—Å–∏–º—É–º 3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è

def calculate_impact_metrics(cluster: Dict[str, Any]) -> Dict[str, Any]:
    """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –≤–ª–∏—è–Ω–∏—è –∫–ª–∞—Å—Ç–µ—Ä–∞"""
    
    variants = cluster.get("variants", [])
    total_mentions = sum(v.get("count", 1) for v in variants)
    
    # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    metrics = {
        "total_mentions": total_mentions,
        "unique_variants": len(variants),
        "avg_mentions_per_variant": total_mentions / len(variants) if variants else 0
    }
    
    # –ê–Ω–∞–ª–∏–∑ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å–≤—è–∑–Ω–æ—Å—Ç–∏
    if cluster.get("semantic_coherence"):
        metrics["semantic_coherence"] = cluster["semantic_coherence"]
    
    # –ê–Ω–∞–ª–∏–∑ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞
    priority = cluster.get("priority", "–Ω–∏–∑–∫–∏–π")
    priority_scores = {"–Ω–∏–∑–∫–∏–π": 1, "—Å—Ä–µ–¥–Ω–∏–π": 2, "–≤—ã—Å–æ–∫–∏–π": 3}
    metrics["priority_score"] = priority_scores.get(priority, 1)
    
    # –û–±—â–∏–π –±–∞–ª–ª –≤–ª–∏—è–Ω–∏—è
    impact_score = (
        metrics["total_mentions"] * 0.4 +
        metrics["priority_score"] * 0.3 +
        metrics.get("semantic_coherence", 0.5) * 0.3
    )
    metrics["impact_score"] = impact_score
    
    return metrics

def analyze_cluster_trends(clusters: List[Dict[str, Any]]) -> Dict[str, Any]:
    """–ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤ –≤ –∫–ª–∞—Å—Ç–µ—Ä–∞—Ö"""
    
    # –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–≤
    priority_distribution = Counter()
    for cluster in clusters:
        priority = cluster.get("priority", "–Ω–∏–∑–∫–∏–π")
        priority_distribution[priority] += 1
    
    # –ê–Ω–∞–ª–∏–∑ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å–≤—è–∑–Ω–æ—Å—Ç–∏
    coherence_scores = [cluster.get("semantic_coherence", 0) for cluster in clusters if cluster.get("semantic_coherence")]
    avg_coherence = sum(coherence_scores) / len(coherence_scores) if coherence_scores else 0
    
    # –ê–Ω–∞–ª–∏–∑ –º–µ—Ç—Ä–∏–∫ –≤–ª–∏—è–Ω–∏—è
    impact_scores = [cluster.get("impact_metrics", {}).get("impact_score", 0) for cluster in clusters]
    avg_impact = sum(impact_scores) / len(impact_scores) if impact_scores else 0
    
    return {
        "priority_distribution": dict(priority_distribution),
        "avg_semantic_coherence": avg_coherence,
        "avg_impact_score": avg_impact,
        "high_impact_clusters": len([c for c in clusters if c.get("impact_metrics", {}).get("impact_score", 0) > 2.0])
    }

def semantic_enrichment(clusters: Dict[str, Any]) -> Dict[str, Any]:
    """–û–±–æ–≥–∞—â–µ–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π"""
    
    logger.info("üîç –ó–∞–ø—É—Å–∫ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–±–æ–≥–∞—â–µ–Ω–∏—è...")
    
    enriched_clusters = {}
    
    for cluster_type, cluster_list in clusters.items():
        if cluster_type in ["barriers", "ideas", "signals"]:
            enriched_list = []
            
            for cluster in cluster_list:
                # –û–±–æ–≥–∞—â–∞–µ–º –∫–ª–∞—Å—Ç–µ—Ä
                enriched_cluster = cluster.copy()
                
                # –î–æ–±–∞–≤–ª—è–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ
                enriched_cluster["description"] = generate_cluster_description(cluster)
                
                # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
                enriched_cluster["priority"] = calculate_cluster_priority(cluster)
                
                # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–æ —Ä–µ—à–µ–Ω–∏—é
                enriched_cluster["solutions"] = generate_solution_suggestions(cluster)
                
                # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –≤–ª–∏—è–Ω–∏—è
                enriched_cluster["impact_metrics"] = calculate_impact_metrics(cluster)
                
                # –î–æ–±–∞–≤–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏
                enriched_cluster["enriched_at"] = pd.Timestamp.now().isoformat()
                
                enriched_list.append(enriched_cluster)
            
            enriched_clusters[cluster_type] = enriched_list
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∞–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤
    all_clusters = []
    for cluster_list in enriched_clusters.values():
        if isinstance(cluster_list, list):
            all_clusters.extend(cluster_list)
    
    enriched_clusters["trend_analysis"] = analyze_cluster_trends(all_clusters)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –æ–±—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏
    enriched_clusters["enrichment_metrics"] = {
        "total_clusters": sum(len(clusters) for clusters in enriched_clusters.values() if isinstance(clusters, list)),
        "enrichment_timestamp": pd.Timestamp.now().isoformat()
    }
    
    logger.info("‚úÖ –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ –æ–±–æ–≥–∞—â–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ")
    
    return enriched_clusters

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    logger.info("üöÄ Stage 4.5: –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ –æ–±–æ–≥–∞—â–µ–Ω–∏–µ")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    input_file = "artifacts/stage4_clusters_enhanced.json"
    if not Path(input_file).exists():
        logger.error(f"–§–∞–π–ª {input_file} –Ω–µ –Ω–∞–π–¥–µ–Ω. –ó–∞–ø—É—Å—Ç–∏—Ç–µ Stage 4 Enhanced —Å–Ω–∞—á–∞–ª–∞.")
        return
    
    with open(input_file, 'r', encoding='utf-8') as f:
        clusters = json.load(f)
    
    logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {len(clusters.get('barriers', [])) + len(clusters.get('ideas', [])) + len(clusters.get('signals', []))}")
    
    # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ –æ–±–æ–≥–∞—â–µ–Ω–∏–µ
    enriched_clusters = semantic_enrichment(clusters)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    output_file = "artifacts/stage4_5_semantic_enrichment.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(enriched_clusters, f, ensure_ascii=False, indent=2)
    
    logger.info(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {output_file}")

if __name__ == "__main__":
    main()
