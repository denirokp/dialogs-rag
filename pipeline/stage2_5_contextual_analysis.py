#!/usr/bin/env python3
"""
Stage 2.5: –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑
–ê–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –∏ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π –¥–∏–Ω–∞–º–∏–∫–∏
"""

import json
import logging
from pathlib import Path
from typing import List, Dict, Any
from collections import Counter, defaultdict
import pandas as pd

import sys
sys.path.append(str(Path(__file__).parent.parent))

from config import settings

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def analyze_problem_sequences(extractions: List[Dict[str, Any]]) -> Dict[str, Any]:
    """–ê–Ω–∞–ª–∏–∑ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–æ–±–ª–µ–º –≤ –¥–∏–∞–ª–æ–≥–∞—Ö"""
    
    sequences = []
    
    for extraction in extractions:
        barriers = extraction.get("barriers", [])
        if len(barriers) > 1:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç—ã –±–∞—Ä—å–µ—Ä–æ–≤
            barrier_texts = []
            for barrier in barriers:
                if isinstance(barrier, dict):
                    barrier_texts.append(barrier["text"])
                else:
                    barrier_texts.append(barrier)
            
            sequences.append({
                "dialog_id": extraction.get("dialog_id", ""),
                "barriers": barrier_texts,
                "sequence_length": len(barrier_texts)
            })
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —á–∞—Å—Ç—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    common_sequences = Counter()
    for seq in sequences:
        if len(seq["barriers"]) >= 2:
            # –°–æ–∑–¥–∞–µ–º –ø–∞—Ä—ã –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –±–∞—Ä—å–µ—Ä–æ–≤
            for i in range(len(seq["barriers"]) - 1):
                pair = (seq["barriers"][i], seq["barriers"][i + 1])
                common_sequences[pair] += 1
    
    return {
        "total_sequences": len(sequences),
        "common_pairs": {str(k): v for k, v in common_sequences.most_common(10)},
        "avg_sequence_length": sum(s["sequence_length"] for s in sequences) / len(sequences) if sequences else 0
    }

def analyze_emotional_dynamics(extractions: List[Dict[str, Any]]) -> Dict[str, Any]:
    """–ê–Ω–∞–ª–∏–∑ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π –¥–∏–Ω–∞–º–∏–∫–∏"""
    
    emotional_states = []
    sentiment_changes = []
    
    for extraction in extractions:
        emotional_state = extraction.get("emotional_state", "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ")
        sentiment = extraction.get("sentiment", "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ")
        
        emotional_states.append(emotional_state)
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
        if "previous_sentiment" in extraction:
            if extraction["previous_sentiment"] != sentiment:
                sentiment_changes.append({
                    "from": extraction["previous_sentiment"],
                    "to": sentiment,
                    "dialog_id": extraction.get("dialog_id", "")
                })
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π
    state_counts = Counter(emotional_states)
    
    return {
        "emotional_distribution": dict(state_counts),
        "sentiment_changes": sentiment_changes,
        "most_common_state": state_counts.most_common(1)[0][0] if state_counts else "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ"
    }

def analyze_decision_impact(extractions: List[Dict[str, Any]]) -> Dict[str, Any]:
    """–ê–Ω–∞–ª–∏–∑ –≤–ª–∏—è–Ω–∏—è –Ω–∞ —Ä–µ—à–µ–Ω–∏–µ –æ –ø–æ–∫—É–ø–∫–µ"""
    
    impact_counts = Counter()
    blocking_issues = []
    
    for extraction in extractions:
        decision_impact = extraction.get("decision_impact", "–Ω–µ –≤–ª–∏—è–µ—Ç")
        impact_counts[decision_impact] += 1
        
        if decision_impact == "–±–ª–æ–∫–∏—Ä—É–µ—Ç –ø–æ–∫—É–ø–∫—É":
            barriers = extraction.get("barriers", [])
            for barrier in barriers:
                if isinstance(barrier, dict):
                    blocking_issues.append({
                        "barrier": barrier["text"],
                        "severity": barrier.get("severity", "—Å—Ä–µ–¥–Ω—è—è"),
                        "dialog_id": extraction.get("dialog_id", "")
                    })
    
    return {
        "impact_distribution": dict(impact_counts),
        "blocking_issues": blocking_issues,
        "blocking_rate": impact_counts["–±–ª–æ–∫–∏—Ä—É–µ—Ç –ø–æ–∫—É–ø–∫—É"] / len(extractions) if extractions else 0
    }

def identify_root_causes(extractions: List[Dict[str, Any]]) -> Dict[str, Any]:
    """–í—ã—è–≤–ª–µ–Ω–∏–µ –∫–æ—Ä–Ω–µ–≤—ã—Ö –ø—Ä–∏—á–∏–Ω –ø—Ä–æ–±–ª–µ–º"""
    
    # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–æ—Ä–Ω–µ–≤—ã—Ö –ø—Ä–∏—á–∏–Ω
    root_cause_keywords = {
        "—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ_–ø—Ä–æ–±–ª–µ–º—ã": ["–Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç", "–æ—à–∏–±–∫–∞", "—Å–±–æ–π", "–Ω–µ –ø–æ–Ω–∏–º–∞—é", "–∫–∞–∫ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å"],
        "–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π_–æ–ø—ã—Ç": ["—Å–ª–æ–∂–Ω–æ", "–Ω–µ—É–¥–æ–±–Ω–æ", "–Ω–µ–ø–æ–Ω—è—Ç–Ω–æ", "–∑–∞–ø—É—Ç–∞–Ω–Ω–æ"],
        "–ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ_–ø—Ä–æ–±–ª–µ–º—ã": ["–Ω–µ –¥–æ—Å—Ç–∞–≤–ª—è—é—Ç", "–¥–æ–ª–≥–æ", "–Ω–µ –≤ –º–æ–π –≥–æ—Ä–æ–¥", "–º–∞–ª–æ –ø–≤–∑"],
        "—Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ_–ø—Ä–æ–±–ª–µ–º—ã": ["–¥–æ—Ä–æ–≥–æ", "—Å–∫—Ä—ã—Ç—ã–µ –ø–ª–∞—Ç–µ–∂–∏", "–Ω–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–µ —Ä–∞—Å—Ö–æ–¥—ã"]
    }
    
    root_causes = defaultdict(int)
    
    for extraction in extractions:
        barriers = extraction.get("barriers", [])
        
        for barrier in barriers:
            barrier_text = barrier["text"] if isinstance(barrier, dict) else barrier
            barrier_text_lower = barrier_text.lower()
            
            for cause_type, keywords in root_cause_keywords.items():
                if any(keyword in barrier_text_lower for keyword in keywords):
                    root_causes[cause_type] += 1
                    break
    
    return {
        "root_causes": dict(root_causes),
        "most_common_cause": max(root_causes.items(), key=lambda x: x[1])[0] if root_causes else "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"
    }

def analyze_expertise_patterns(extractions: List[Dict[str, Any]]) -> Dict[str, Any]:
    """–ê–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ—Å—Ç–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π"""
    
    expertise_levels = []
    expertise_barriers = defaultdict(list)
    
    for extraction in extractions:
        expertise_level = extraction.get("expertise_level", "—Å—Ä–µ–¥–Ω–∏–π")
        expertise_levels.append(expertise_level)
        
        barriers = extraction.get("barriers", [])
        for barrier in barriers:
            barrier_text = barrier["text"] if isinstance(barrier, dict) else barrier
            expertise_barriers[expertise_level].append(barrier_text)
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–∏–ø—ã –ø—Ä–æ–±–ª–µ–º –ø–æ —É—Ä–æ–≤–Ω—è–º —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ—Å—Ç–∏
    expertise_analysis = {}
    for level, barriers in expertise_barriers.items():
        if barriers:
            # –ù–∞—Ö–æ–¥–∏–º –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç—ã–µ –ø—Ä–æ–±–ª–µ–º—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —É—Ä–æ–≤–Ω—è
            barrier_counts = Counter(barriers)
            most_common = barrier_counts.most_common(5)
            expertise_analysis[level] = {
                "total_barriers": len(barriers),
                "unique_barriers": len(set(barriers)),
                "most_common": {str(k): v for k, v in most_common}
            }
    
    return {
        "expertise_distribution": dict(Counter(expertise_levels)),
        "expertise_analysis": expertise_analysis
    }

def contextual_analysis(extractions: List[Dict[str, Any]]) -> Dict[str, Any]:
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞"""
    
    logger.info("üîç –ó–∞–ø—É—Å–∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞...")
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ –∞—Å–ø–µ–∫—Ç—ã
    problem_sequences = analyze_problem_sequences(extractions)
    emotional_dynamics = analyze_emotional_dynamics(extractions)
    decision_impact = analyze_decision_impact(extractions)
    root_causes = identify_root_causes(extractions)
    expertise_patterns = analyze_expertise_patterns(extractions)
    
    result = {
        "problem_sequences": problem_sequences,
        "emotional_dynamics": emotional_dynamics,
        "decision_impact": decision_impact,
        "root_causes": root_causes,
        "expertise_patterns": expertise_patterns,
        "analysis_timestamp": pd.Timestamp.now().isoformat()
    }
    
    logger.info("‚úÖ –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω")
    
    return result

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    logger.info("üöÄ Stage 2.5: –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    input_file = "artifacts/stage2_extracted_enhanced.jsonl"
    if not Path(input_file).exists():
        logger.error(f"–§–∞–π–ª {input_file} –Ω–µ –Ω–∞–π–¥–µ–Ω. –ó–∞–ø—É—Å—Ç–∏—Ç–µ Stage 2 Enhanced —Å–Ω–∞—á–∞–ª–∞.")
        return
    
    extractions = []
    with open(input_file, 'r', encoding='utf-8') as f:
        for line in f:
            extractions.append(json.loads(line))
    
    logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(extractions)} –∏–∑–≤–ª–µ—á–µ–Ω–∏–π")
    
    # –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑
    context_analysis = contextual_analysis(extractions)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    output_file = "artifacts/stage2_5_contextual_analysis.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(context_analysis, f, ensure_ascii=False, indent=2)
    
    logger.info(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {output_file}")

if __name__ == "__main__":
    main()
