#!/usr/bin/env python3
"""
–ï–¥–∏–Ω—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –∏ –∞–Ω–∞–ª–∏–∑–∞ –¥–∏–∞–ª–æ–≥–æ–≤
"""
import os
import json
import time
import csv
from typing import List, Dict, Any

import pandas as pd
import chromadb
from sentence_transformers import SentenceTransformer
from openai import OpenAI
from dotenv import load_dotenv

from config import settings
from prompts import ANALYST_SYSTEM_PROMPT, DEFAULT_QUERY

load_dotenv()

def read_excel(xlsx_path: str, sheet_names=None):
    """–ß–∏—Ç–∞–µ—Ç Excel —Ñ–∞–π–ª"""
    if sheet_names is None:
        return pd.read_excel(xlsx_path)
    else:
        return pd.concat([pd.read_excel(xlsx_path, sheet_name=name) for name in sheet_names], ignore_index=True)

def split_turns(text: str):
    """–†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —Ä–µ–ø–ª–∏–∫–∏ –ø–æ –º–µ—Ç–∫–∞–º —Ä–æ–ª–µ–π"""
    if not text or pd.isna(text):
        return []
    
    # –ò—â–µ–º –º–µ—Ç–∫–∏ —Ä–æ–ª–µ–π
    client_label = settings.client_label
    operator_label = settings.operator_label
    
    turns = []
    lines = text.split('\n')
    current_turn = ""
    current_role = None
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
            
        if client_label in line.lower():
            if current_turn and current_role:
                turns.append({"role": current_role, "text": current_turn.strip()})
            current_turn = line
            current_role = "client"
        elif operator_label in line.lower():
            if current_turn and current_role:
                turns.append({"role": current_role, "text": current_turn.strip()})
            current_turn = line
            current_role = "operator"
        else:
            if current_turn:
                current_turn += " " + line
            else:
                current_turn = line
    
    if current_turn and current_role:
        turns.append({"role": current_role, "text": current_turn.strip()})
    
    return turns

def build_windows(dialog_id: str, turns, prev: int, next_: int):
    """–°–æ–∑–¥–∞–µ—Ç –æ–∫–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤–æ–∫—Ä—É–≥ –∫–ª–∏–µ–Ω—Ç—Å–∫–∏—Ö —Ä–µ–ø–ª–∏–∫"""
    windows = []
    
    for i, turn in enumerate(turns):
        if turn["role"] != "client":
            continue
            
        # –ì—Ä–∞–Ω–∏—Ü—ã –æ–∫–Ω–∞
        start = max(0, i - prev)
        end = min(len(turns), i + next_ + 1)
        
        # –°–æ–±–∏—Ä–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
        context_turns = turns[start:end]
        context_full = " ".join([t["text"] for t in context_turns])
        context_client_only = " ".join([t["text"] for t in context_turns if t["role"] == "client"])
        
        window_id = f"{dialog_id}_{i}"
        
        windows.append({
            "id": window_id,
            "dialog_id": dialog_id,
            "turn_L": start,
            "turn_R": end - 1,
            "context_full": context_full,
            "context_client_only": context_client_only
        })
    
    return windows

def retrieve_for_dialog(dialog_id: str, query: str, topN: int = 15, emb_model=None, collection=None):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –±–ª–æ–∫–∏ –¥–ª—è –¥–∏–∞–ª–æ–≥–∞"""
    qvec = emb_model.encode([query], normalize_embeddings=True).tolist()[0]
    
    res = collection.query(
        query_embeddings=[qvec],
        n_results=topN,
        where={"dialog_id": dialog_id}
    )
    
    blocks = []
    for i in range(len(res["ids"][0])):
        meta = res["metadatas"][0][i]
        blocks.append({
            "id": res["ids"][0][i],
            "dialog_id": meta["dialog_id"],
            "turn_L": meta["turn_L"],
            "turn_R": meta["turn_R"],
            "context_full": res["documents"][0][i],
            "context_client_only": meta["client_only"],
            "distance": res["distances"][0][i]
        })
    
    return blocks

def rerank_blocks(query: str, blocks: List[Dict], top_k: int):
    """–†–∞–Ω–∂–∏—Ä—É–µ—Ç –±–ª–æ–∫–∏ –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏"""
    if not blocks:
        return []
    
    # –ü—Ä–æ—Å—Ç–æ–µ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—é
    sorted_blocks = sorted(blocks, key=lambda x: x["distance"])
    return sorted_blocks[:top_k]

def call_llm(system_prompt: str, user_prompt: str):
    """–í—ã–∑—ã–≤–∞–µ—Ç LLM"""
    if not settings.use_openai:
        return '{"delivery_types": [], "barriers": [], "ideas": [], "self_check": "LLM –æ—Ç–∫–ª—é—á–µ–Ω"}'
    
    oai = OpenAI()
    response = oai.chat.completions.create(
        model=settings.openai_model,
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ],
        temperature=0.1
    )
    return response.choices[0].message.content

def has_client_citations(payload):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ —Ü–∏—Ç–∞—Ç –∫–ª–∏–µ–Ω—Ç–∞ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö"""
    text = json.dumps(payload, ensure_ascii=False)
    return settings.client_label in text.lower()

def main():
    print("üöÄ –ó–∞–ø—É—Å–∫–∞–µ–º –ø–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–∏–∞–ª–æ–≥–æ–≤...")
    
    # 1. –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
    print("\nüìä –≠—Ç–∞–ø 1: –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö...")
    df = read_excel(settings.xlsx_path, settings.sheet_names).fillna("")
    assert settings.col_id in df.columns, f"–ù–µ—Ç –∫–æ–ª–æ–Ω–∫–∏: {settings.col_id}"
    assert settings.col_text in df.columns, f"–ù–µ—Ç –∫–æ–ª–æ–Ω–∫–∏: {settings.col_text}"
    
    print(f"üìà –ù–∞–π–¥–µ–Ω–æ –¥–∏–∞–ª–æ–≥–æ–≤: {len(df)}")
    print("üîç –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–∏–∞–ª–æ–≥–∏...")

    records = []
    processed = 0
    for _, row in df.iterrows():
        did = row[settings.col_id]
        turns = split_turns(row[settings.col_text])
        if not turns:
            continue
        records.extend(build_windows(str(did), turns, settings.prev_turns, settings.next_turns))
        processed += 1
        if processed % 100 == 0:
            print(f"  –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –¥–∏–∞–ª–æ–≥–æ–≤: {processed}/{len(df)}")

    if not records:
        print("‚ùå –ù–µ—á–µ–≥–æ –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å.")
        return

    print(f"üì¶ –°–æ–∑–¥–∞–Ω–æ –æ–∫–æ–Ω: {len(records)}")
    print("ü§ñ –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")
    emb = SentenceTransformer(settings.embed_model_name)
    
    print("üî¢ –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏...")
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –±–∞—Ç—á–∞–º–∏ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
    batch_size = 100
    all_embeddings = []
    
    for i in range(0, len(records), batch_size):
        batch = records[i:i + batch_size]
        batch_texts = [r["context_full"] for r in batch]
        batch_embeddings = emb.encode(
            batch_texts,
            normalize_embeddings=True, 
            show_progress_bar=True,
            batch_size=32
        )
        all_embeddings.extend(batch_embeddings.tolist())
        print(f"  –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –±–∞—Ç—á–µ–π: {i//batch_size + 1}/{(len(records)-1)//batch_size + 1}")

    print("üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ ChromaDB...")
    ch = chromadb.Client()
    col = ch.create_collection(name=settings.collection)

    # –î–æ–±–∞–≤–ª—è–µ–º –±–∞—Ç—á–∞–º–∏ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
    add_batch_size = 500
    for i in range(0, len(records), add_batch_size):
        batch = records[i:i + add_batch_size]
        batch_embeddings = all_embeddings[i:i + add_batch_size]
        
        col.add(
            ids=[r["id"] for r in batch],
            documents=[r["context_full"] for r in batch],
            metadatas=[{
                "dialog_id": r["dialog_id"],
                "turn_L": r["turn_L"],
                "turn_R": r["turn_R"],
                "client_only": r["context_client_only"][:20000]
            } for r in batch],
            embeddings=batch_embeddings
        )
        print(f"  –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –±–∞—Ç—á–µ–π: {i//add_batch_size + 1}/{(len(records)-1)//add_batch_size + 1}")
    
    print(f"‚úÖ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞! –ü—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–æ –æ–∫–æ–Ω: {len(records)}")
    
    # 2. –ê–Ω–∞–ª–∏–∑ –¥–∏–∞–ª–æ–≥–æ–≤
    print("\nüìä –≠—Ç–∞–ø 2: –ê–Ω–∞–ª–∏–∑ –¥–∏–∞–ª–æ–≥–æ–≤...")
    
    # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ ID –¥–∏–∞–ª–æ–≥–æ–≤
    res = col.get(include=["metadatas"], limit=200000)
    dialog_ids = sorted({m["dialog_id"] for m in res["metadatas"]})
    
    print(f"üìä –î–∏–∞–ª–æ–≥–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {len(dialog_ids)}")
    print(f"‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏: top_k={settings.top_k}, –º–æ–¥–µ–ª—å={settings.openai_model}")
    print("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –∞–Ω–∞–ª–∏–∑...")

    with open("batch_results.jsonl", "w", encoding="utf-8") as jf, \
         open("batch_results.csv", "w", encoding="utf-8", newline="") as cf:
        
        csv_w = csv.DictWriter(cf, fieldnames=[
            "dialog_id", "delivery_types", "barriers", "ideas", "self_check"
        ])
        csv_w.writeheader()

        processed = 0
        errors = 0
        
        for i, did in enumerate(dialog_ids, 1):
            try:
                # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–∏—Å–∫
                blocksN = retrieve_for_dialog(did, DEFAULT_QUERY, topN=15, emb_model=emb, collection=col)
                blocks = rerank_blocks(DEFAULT_QUERY, blocksN, settings.top_k)
                
                if not blocks:
                    payload = {
                        "dialog_id": did,
                        "delivery_types": [],
                        "barriers": [],
                        "ideas": [],
                        "self_check": "–ù–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –æ–∫–æ–Ω"
                    }
                else:
                    # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç
                    context = "\n\n".join([f"–û–∫–Ω–æ {j+1}:\n{b['context_full']}" for j, b in enumerate(blocks)])
                    prompt = f"–ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–∏–∞–ª–æ–≥–∞:\n{context}\n\n–ó–∞–ø—Ä–æ—Å: {DEFAULT_QUERY}"
                    
                    # –í—ã–∑—ã–≤–∞–µ–º LLM
                    ans_raw = call_llm(ANALYST_SYSTEM_PROMPT, prompt)
                    
                    try:
                        data = json.loads(ans_raw)
                    except json.JSONDecodeError:
                        # –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–º –ø—Ä–æ–º–ø—Ç–æ–º
                        ans_raw = call_llm(
                            ANALYST_SYSTEM_PROMPT + "\n–û—Ç–≤–µ—á–∞–π —Å—Ç—Ä–æ–≥–æ –≤–∞–ª–∏–¥–Ω—ã–º JSON –±–µ–∑ –ø—Ä–µ–∞–º–±—É–ª—ã.",
                            prompt
                        )
                        data = json.loads(ans_raw)
                    payload = {"dialog_id": did, **data}

                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ —Ü–∏—Ç–∞—Ç
                    if (payload.get("delivery_types") or payload.get("barriers") or payload.get("ideas")) and not has_client_citations(payload):                                                                       
                        sc = payload.get("self_check","")
                        payload["self_check"] = (sc + " | –ù–µ—Ç —Ü–∏—Ç–∞—Ç –∫–ª–∏–µ–Ω—Ç–∞ –¥–ª—è —á–∞—Å—Ç–∏ –≤—ã–≤–æ–¥–æ–≤").strip()

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                jf.write(json.dumps(payload, ensure_ascii=False) + "\n")
                csv_w.writerow({
                    "dialog_id": payload["dialog_id"],
                    "delivery_types": json.dumps(payload.get("delivery_types", []), ensure_ascii=False),
                    "barriers": json.dumps(payload.get("barriers", []), ensure_ascii=False),
                    "ideas": json.dumps(payload.get("ideas", []), ensure_ascii=False),
                    "self_check": payload.get("self_check","")
                })

                processed += 1
                
                # –ü—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–µ 25 –¥–∏–∞–ª–æ–≥–æ–≤
                if i % 25 == 0:
                    print(f"üìà –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {i}/{len(dialog_ids)} ({i/len(dialog_ids)*100:.1f}%)")
                    print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ: {processed}, ‚ùå –û—à–∏–±–æ–∫: {errors}")
                
                # –£–º–µ–Ω—å—à–µ–Ω–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
                time.sleep(0.02)

            except Exception as e:
                errors += 1
                print(f"‚ùå [{did}] –û—à–∏–±–∫–∞: {e}")
                jf.write(json.dumps({"dialog_id": did, "error": str(e)}, ensure_ascii=False) + "\n")
                continue
        
        print(f"\nüéâ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω!")
        print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {processed} —É—Å–ø–µ—à–Ω–æ, {errors} –æ—à–∏–±–æ–∫ –∏–∑ {len(dialog_ids)} –¥–∏–∞–ª–æ–≥–æ–≤")
        print(f"üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ batch_results.csv –∏ batch_results.jsonl")

if __name__ == "__main__":
    main()
