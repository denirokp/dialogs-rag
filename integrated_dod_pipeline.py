#!/usr/bin/env python3
"""
üéØ –ò–ù–¢–ï–ì–†–ò–†–û–í–ê–ù–ù–´–ô –ü–ê–ô–ü–õ–ê–ô–ù DIALOGS RAG SYSTEM —Å DoD
–ü–æ–ª–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –∞–Ω–∞–ª–∏–∑–∞ –¥–∏–∞–ª–æ–≥–æ–≤ —Å —Å–æ–±–ª—é–¥–µ–Ω–∏–µ–º Definition of Done
"""

import asyncio
import json
import logging
import time
import os
import sys
from pathlib import Path
from typing import Dict, List, Any, Optional
import pandas as pd
import numpy as np
from datetime import datetime
import argparse
import yaml
import jsonschema
import duckdb
from collections import defaultdict

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç–∏ –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞
sys.path.append(str(Path(__file__).parent))

# –ò–º–ø–æ—Ä—Ç –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ DoD
from scripts.dedup import main as dedup_main
from scripts.clusterize import main as clusterize_main
from scripts.eval_extraction import micro_f1
from quality.run_checks import main as run_quality_checks

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/integrated_dod_pipeline.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class IntegratedDoDPipeline:
    """–ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω —Å —Å–æ–±–ª—é–¥–µ–Ω–∏–µ–º DoD"""
    
    def __init__(self, config_path: str = "final_pipeline_config.json"):
        self.config = self._load_config(config_path)
        self.taxonomy = self._load_taxonomy()
        self.schema = self._load_schema()
        self.results = []
        self.statistics = {}
        
        # –°–æ–∑–¥–∞–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        self._create_directories()
        
        logger.info("üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ DoD –ø–∞–π–ø–ª–∞–π–Ω–∞...")
    
    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        if Path(config_path).exists():
            with open(config_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        else:
            return {
                "openai_api_key": "your-api-key-here",
                "processing": {
                    "enable_validation": True,
                    "enable_dedup": True,
                    "enable_clustering": True,
                    "enable_quality_checks": True,
                    "max_dialogs_per_batch": 1000,
                    "quality_threshold": 0.6
                },
                "dedup": {
                    "threshold": 0.92,
                    "enable_embeddings": False
                },
                "clustering": {
                    "min_cluster_size": 25,
                    "n_neighbors": 12,
                    "min_dist": 0.1
                }
            }
    
    def _load_taxonomy(self) -> Dict[str, Any]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Ç–∞–∫—Å–æ–Ω–æ–º–∏–∏"""
        with open('taxonomy.yaml', 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    
    def _load_schema(self) -> Dict[str, Any]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ JSON —Å—Ö–µ–º—ã"""
        with open('schemas/mentions.schema.json', 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def _create_directories(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π"""
        dirs = ['logs', 'artifacts', 'reports', 'goldset', 'quality', 'sql', 'scripts', 'schemas']
        for dir_name in dirs:
            Path(dir_name).mkdir(exist_ok=True)
    
    async def process_dialogs(self, dialogs: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∏–∞–ª–æ–≥–æ–≤ —Å —Å–æ–±–ª—é–¥–µ–Ω–∏–µ–º DoD"""
        logger.info(f"üìä –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É {len(dialogs)} –¥–∏–∞–ª–æ–≥–æ–≤ —Å DoD...")
        
        start_time = time.time()
        
        # Stage 1: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π (client-only + evidence)
        logger.info("üîç Stage 1: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π...")
        mentions = await self._extract_mentions(dialogs)
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è –ø–æ JSON —Å—Ö–µ–º–µ
        if self.config["processing"]["enable_validation"]:
            logger.info("‚úÖ –í–∞–ª–∏–¥–∞—Ü–∏—è –ø–æ JSON —Å—Ö–µ–º–µ...")
            self._validate_mentions(mentions)
        
        # Stage 2: –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è
        if self.config["processing"]["enable_dedup"]:
            logger.info("üîÑ Stage 2: –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è...")
            mentions = await self._deduplicate_mentions(mentions)
        
        # Stage 3: –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
        if self.config["processing"]["enable_clustering"]:
            logger.info("üéØ Stage 3: –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è...")
            clusters = await self._cluster_mentions(mentions)
        else:
            clusters = {}
        
        # Stage 4: –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å–≤–æ–¥–æ–∫
        logger.info("üìä Stage 4: –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å–≤–æ–¥–æ–∫...")
        summaries = await self._build_summaries(mentions)
        
        # Stage 5: –ü—Ä–æ–≤–µ—Ä–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ DoD
        if self.config["processing"]["enable_quality_checks"]:
            logger.info("üîç Stage 5: –ü—Ä–æ–≤–µ—Ä–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ DoD...")
            quality_results = await self._run_quality_checks(mentions)
        else:
            quality_results = {}
        
        # Stage 6: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–æ–≤
        logger.info("üìù Stage 6: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–æ–≤...")
        reports = await self._generate_reports(mentions, clusters, summaries)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        processing_time = time.time() - start_time
        self.statistics = {
            "total_dialogs": len(dialogs),
            "total_mentions": len(mentions),
            "processing_time_seconds": processing_time,
            "mentions_per_second": len(mentions) / processing_time if processing_time > 0 else 0,
            "quality_results": quality_results,
            "clusters_found": len(clusters),
            "summaries_generated": len(summaries)
        }
        
        self.results = {
            "mentions": mentions,
            "clusters": clusters,
            "summaries": summaries,
            "quality_results": quality_results,
            "reports": reports,
            "statistics": self.statistics
        }
        
        logger.info(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {processing_time:.2f} —Å–µ–∫—É–Ω–¥")
        logger.info(f"üìà –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π: {len(mentions)}")
        logger.info(f"üéØ –ù–∞–π–¥–µ–Ω–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {len(clusters)}")
        
        return self.results
    
    async def _extract_mentions(self, dialogs: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π —Ç–æ–ª—å–∫–æ –∏–∑ —Ä–µ–ø–ª–∏–∫ –∫–ª–∏–µ–Ω—Ç–∞"""
        mentions = []
        
        for dialog_idx, dialog in enumerate(dialogs):
            dialog_id = dialog.get("dialog_id", dialog_idx)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ —Ä–µ–ø–ª–∏–∫–∏ –∫–ª–∏–µ–Ω—Ç–∞
            client_turns = []
            if "turns" in dialog:
                for turn_idx, turn in enumerate(dialog["turns"]):
                    if turn.get("role") == "client":
                        client_turns.append((turn_idx, turn))
            elif "messages" in dialog:
                for turn_idx, message in enumerate(dialog["messages"]):
                    if message.get("role") == "client":
                        client_turns.append((turn_idx, message))
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –∏–∑ –∫–∞–∂–¥–æ–π —Ä–µ–ø–ª–∏–∫–∏ –∫–ª–∏–µ–Ω—Ç–∞
            for turn_idx, turn in client_turns:
                text = turn.get("text", "")
                if not text.strip():
                    continue
                
                # –ü—Ä–æ—Å—Ç–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
                extracted_mentions = self._extract_mentions_from_text(text, dialog_id, turn_idx)
                mentions.extend(extracted_mentions)
        
        return mentions
    
    def _extract_mentions_from_text(self, text: str, dialog_id: int, turn_id: int) -> List[Dict[str, Any]]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π –∏–∑ —Ç–µ–∫—Å—Ç–∞ –∫–ª–∏–µ–Ω—Ç–∞"""
        mentions = []
        
        # –ü—Ä–æ—Å—Ç–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º –∏–∑ —Ç–∞–∫—Å–æ–Ω–æ–º–∏–∏
        for theme in self.taxonomy["themes"]:
            theme_name = theme["name"]
            theme_id = theme["id"]
            
            for subtheme in theme["subthemes"]:
                subtheme_name = subtheme["name"]
                subtheme_id = subtheme["id"]
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –ø–æ–¥—Ç–µ–º—ã –≤ —Ç–µ–∫—Å—Ç–µ
                keywords = subtheme_name.lower().split()
                if any(keyword in text.lower() for keyword in keywords):
                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –º–µ—Ç–∫–∏
                    label_type = self._determine_label_type(text, subtheme_name)
                    
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ü–∏—Ç–∞—Ç—É
                    quote = self._extract_quote(text, keywords)
                    
                    if quote:
                        mention = {
                            "dialog_id": dialog_id,
                            "turn_id": turn_id,
                            "theme": theme_name,
                            "subtheme": subtheme_name,
                            "label_type": label_type,
                            "text_quote": quote,
                            "delivery_type": self._extract_delivery_type(text),
                            "cause_hint": self._extract_cause_hint(text),
                            "confidence": self._calculate_confidence(text, subtheme_name)
                        }
                        mentions.append(mention)
        
        return mentions
    
    def _determine_label_type(self, text: str, subtheme: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –º–µ—Ç–∫–∏"""
        text_lower = text.lower()
        
        if any(word in text_lower for word in ["–ø—Ä–æ–±–ª–µ–º–∞", "–Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç", "—Å–ª–æ–º–∞–ª", "–æ—à–∏–±–∫–∞"]):
            return "–±–∞—Ä—å–µ—Ä"
        elif any(word in text_lower for word in ["–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ", "–∏–¥–µ—è", "–º–æ–∂–Ω–æ", "–ª—É—á—à–µ"]):
            return "–∏–¥–µ—è"
        elif any(word in text_lower for word in ["—Å–∏–≥–Ω–∞–ª", "—É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ", "–∞–ª–µ—Ä—Ç"]):
            return "—Å–∏–≥–Ω–∞–ª"
        elif any(word in text_lower for word in ["—Å–ø–∞—Å–∏–±–æ", "–æ—Ç–ª–∏—á–Ω–æ", "—Ö–æ—Ä–æ—à–æ", "–ø–æ–Ω—Ä–∞–≤–∏–ª–æ—Å—å"]):
            return "–ø–æ—Ö–≤–∞–ª–∞"
        else:
            return "–±–∞—Ä—å–µ—Ä"  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
    
    def _extract_quote(self, text: str, keywords: List[str]) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ü–∏—Ç–∞—Ç—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        sentences = text.split('.')
        for sentence in sentences:
            sentence = sentence.strip()
            if len(sentence) > 10 and any(keyword in sentence.lower() for keyword in keywords):
                return sentence
        return text[:100] + "..." if len(text) > 100 else text
    
    def _extract_delivery_type(self, text: str) -> Optional[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–∏–ø–∞ –¥–æ—Å—Ç–∞–≤–∫–∏"""
        text_lower = text.lower()
        if "–∂–∞–ª–æ–±–∞" in text_lower:
            return "complaint"
        elif "–∑–∞–ø—Ä–æ—Å" in text_lower:
            return "request"
        elif "–≤–æ–ø—Ä–æ—Å" in text_lower:
            return "question"
        elif "–æ—Ç–∑—ã–≤" in text_lower:
            return "feedback"
        return None
    
    def _extract_cause_hint(self, text: str) -> Optional[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–æ–¥—Å–∫–∞–∑–∫–∏ –æ –ø—Ä–∏—á–∏–Ω–µ"""
        text_lower = text.lower()
        if "–∏–∑-–∑–∞" in text_lower:
            return "–ø—Ä–∏—á–∏–Ω–∞ —É–∫–∞–∑–∞–Ω–∞"
        elif "–ø–æ—Ç–æ–º—É —á—Ç–æ" in text_lower:
            return "–ø—Ä–∏—á–∏–Ω–∞ —É–∫–∞–∑–∞–Ω–∞"
        return None
    
    def _calculate_confidence(self, text: str, subtheme: str) -> float:
        """–†–∞—Å—á–µ—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏"""
        text_lower = text.lower()
        subtheme_lower = subtheme.lower()
        
        # –ë—É–∫–≤–∞–ª—å–Ω–∞—è —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞
        if subtheme_lower in text_lower:
            return 0.95
        
        # –û–¥–Ω–æ–∑–Ω–∞—á–Ω—ã–π –ø–µ—Ä–µ—Ñ—Ä–∞–∑
        keywords = subtheme_lower.split()
        if sum(1 for kw in keywords if kw in text_lower) >= len(keywords) * 0.7:
            return 0.85
        
        # –ö–æ—Å–≤–µ–Ω–Ω–æ
        if any(kw in text_lower for kw in keywords):
            return 0.70
        
        # –°–æ–º–Ω–∏—Ç–µ–ª—å–Ω–æ
        return 0.50
    
    def _validate_mentions(self, mentions: List[Dict[str, Any]]):
        """–í–∞–ª–∏–¥–∞—Ü–∏—è —É–ø–æ–º–∏–Ω–∞–Ω–∏–π –ø–æ JSON —Å—Ö–µ–º–µ"""
        try:
            jsonschema.validate(mentions, self.schema)
            logger.info("‚úÖ –í—Å–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –ø—Ä–æ—à–ª–∏ –≤–∞–ª–∏–¥–∞—Ü–∏—é –ø–æ —Å—Ö–µ–º–µ")
        except jsonschema.ValidationError as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {e}")
            raise
    
    async def _deduplicate_mentions(self, mentions: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """–î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è —É–ø–æ–º–∏–Ω–∞–Ω–∏–π"""
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
        temp_file = "artifacts/temp_mentions.jsonl"
        with open(temp_file, 'w', encoding='utf-8') as f:
            for mention in mentions:
                f.write(json.dumps(mention, ensure_ascii=False) + '\n')
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—é
        dedup_file = "artifacts/mentions_dedup.jsonl"
        sys.argv = ['dedup.py', '--in', temp_file, '--out', dedup_file]
        dedup_main()
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        deduped_mentions = []
        with open(dedup_file, 'r', encoding='utf-8') as f:
            for line in f:
                deduped_mentions.append(json.loads(line))
        
        # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
        os.remove(temp_file)
        os.remove(dedup_file)
        
        logger.info(f"üîÑ –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è: {len(mentions)} -> {len(deduped_mentions)} —É–ø–æ–º–∏–Ω–∞–Ω–∏–π")
        return deduped_mentions
    
    async def _cluster_mentions(self, mentions: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —É–ø–æ–º–∏–Ω–∞–Ω–∏–π"""
        clusters = {}
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –ø–æ–¥—Ç–µ–º–∞–º
        by_subtheme = defaultdict(list)
        for mention in mentions:
            key = f"{mention['theme']}_{mention['subtheme']}"
            by_subtheme[key].append(mention)
        
        # –ö–ª–∞—Å—Ç–µ—Ä–∏–∑—É–µ–º –∫–∞–∂–¥—É—é –ø–æ–¥—Ç–µ–º—É
        for subtheme_key, subtheme_mentions in by_subtheme.items():
            if len(subtheme_mentions) < 5:  # –°–ª–∏—à–∫–æ–º –º–∞–ª–æ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
                continue
            
            # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ (–∑–∞–≥–ª—É—à–∫–∞)
            embeddings = np.random.rand(len(subtheme_mentions), 50)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
            temp_mentions = "artifacts/temp_cluster_mentions.jsonl"
            temp_embeddings = "artifacts/temp_embeddings.npy"
            
            with open(temp_mentions, 'w', encoding='utf-8') as f:
                for mention in subtheme_mentions:
                    f.write(json.dumps(mention, ensure_ascii=False) + '\n')
            
            np.save(temp_embeddings, embeddings)
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é
            theme, subtheme = subtheme_key.split('_', 1)
            cluster_file = f"artifacts/clusters_{subtheme_key}.json"
            
            sys.argv = ['clusterize.py', '--mentions', temp_mentions, '--embeddings', temp_embeddings,
                       '--theme', theme, '--subtheme', subtheme, '--out', cluster_file]
            clusterize_main()
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            if Path(cluster_file).exists():
                with open(cluster_file, 'r', encoding='utf-8') as f:
                    clusters[subtheme_key] = json.load(f)
            
            # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
            os.remove(temp_mentions)
            os.remove(temp_embeddings)
            if Path(cluster_file).exists():
                os.remove(cluster_file)
        
        logger.info(f"üéØ –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è: –Ω–∞–π–¥–µ–Ω–æ {len(clusters)} –≥—Ä—É–ø–ø –∫–ª–∞—Å—Ç–µ—Ä–æ–≤")
        return clusters
    
    async def _build_summaries(self, mentions: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å–≤–æ–¥–æ–∫"""
        # –°–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—ã –≤ DuckDB
        conn = duckdb.connect(':memory:')
        
        # –°–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—É –¥–∏–∞–ª–æ–≥–æ–≤
        dialog_ids = list(set(m['dialog_id'] for m in mentions))
        dialogs_df = pd.DataFrame({'dialog_id': dialog_ids})
        conn.register('dialogs', dialogs_df)
        
        # –°–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—É —É–ø–æ–º–∏–Ω–∞–Ω–∏–π
        mentions_df = pd.DataFrame(mentions)
        conn.register('mentions', mentions_df)
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º SQL –∑–∞–ø—Ä–æ—Å—ã
        with open('sql/build_summaries.sql', 'r', encoding='utf-8') as f:
            sql_queries = f.read().split(';')
        
        summaries = {}
        for query in sql_queries:
            query = query.strip()
            if not query:
                continue
            
            try:
                result = conn.execute(query).fetchdf()
                table_name = query.split('CREATE OR REPLACE TABLE')[1].split('AS')[0].strip()
                summaries[table_name] = result.to_dict('records')
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è SQL: {e}")
        
        conn.close()
        
        logger.info(f"üìä –°–≤–æ–¥–∫–∏: —Å–æ–∑–¥–∞–Ω–æ {len(summaries)} —Ç–∞–±–ª–∏—Ü")
        return summaries
    
    async def _run_quality_checks(self, mentions: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–ó–∞–ø—É—Å–∫ –ø—Ä–æ–≤–µ—Ä–æ–∫ –∫–∞—á–µ—Å—Ç–≤–∞ DoD"""
        # –°–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—ã –≤ DuckDB –¥–ª—è –ø—Ä–æ–≤–µ—Ä–æ–∫
        conn = duckdb.connect(':memory:')
        
        # –°–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—É –¥–∏–∞–ª–æ–≥–æ–≤
        dialog_ids = list(set(m['dialog_id'] for m in mentions))
        dialogs_df = pd.DataFrame({'dialog_id': dialog_ids})
        conn.register('dialogs', dialogs_df)
        
        # –°–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—É —É–ø–æ–º–∏–Ω–∞–Ω–∏–π
        mentions_df = pd.DataFrame(mentions)
        conn.register('mentions', mentions_df)
        
        # –°–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—É utterances (–∑–∞–≥–ª—É—à–∫–∞)
        utterances_data = []
        for mention in mentions:
            utterances_data.append({
                'dialog_id': mention['dialog_id'],
                'turn_id': mention['turn_id'],
                'role': 'client'  # –í—Å–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è —Ç–æ–ª—å–∫–æ –æ—Ç –∫–ª–∏–µ–Ω—Ç–∞
            })
        utterances_df = pd.DataFrame(utterances_data)
        conn.register('utterances', utterances_df)
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
        with open('quality/checks.sql', 'r', encoding='utf-8') as f:
            queries = [q.strip() for q in f.read().split(';') if q.strip()]
        
        quality_results = {}
        for i, query in enumerate(queries):
            try:
                result = conn.execute(query).fetchone()
                if i == 0:  # Q1 Evidence-100
                    quality_results['empty_quotes'] = result[0]
                elif i == 1:  # Q2 Client-only-100
                    quality_results['non_client_mentions'] = result[0]
                elif i == 2:  # Q3 Dedup
                    quality_results['dup_pct'] = result[0]
                elif i == 3:  # Q4 Coverage
                    quality_results['misc_share_pct'] = result[0]
                elif i == 4:  # Ambiguity report
                    quality_results['ambiguity_report'] = conn.execute(query).fetchdf().to_dict('records')
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ {i+1}: {e}")
        
        conn.close()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ DoD
        dod_status = {
            "evidence_100": quality_results.get('empty_quotes', 0) == 0,
            "client_only_100": quality_results.get('non_client_mentions', 0) == 0,
            "dedup_1pct": quality_results.get('dup_pct', 0) <= 1.0,
            "coverage_98pct": quality_results.get('misc_share_pct', 0) <= 2.0
        }
        
        quality_results['dod_status'] = dod_status
        quality_results['dod_passed'] = all(dod_status.values())
        
        logger.info(f"üîç –ü—Ä–æ–≤–µ—Ä–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞: DoD {'‚úÖ –ø—Ä–æ–π–¥–µ–Ω' if quality_results['dod_passed'] else '‚ùå –Ω–µ –ø—Ä–æ–π–¥–µ–Ω'}")
        return quality_results
    
    async def _generate_reports(self, mentions: List[Dict[str, Any]], 
                               clusters: Dict[str, Any], 
                               summaries: Dict[str, Any]) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–æ–≤"""
        reports = {}
        
        # –û—Å–Ω–æ–≤–Ω–æ–π –æ—Ç—á–µ—Ç
        total_dialogs = len(set(m['dialog_id'] for m in mentions))
        themes_summary = summaries.get('summary_themes', [])
        subthemes_summary = summaries.get('summary_subthemes', [])
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç
        report_content = f"""# –û—Ç—á–µ—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É –¥–∏–∞–ª–æ–≥–æ–≤

## –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
- –í—Å–µ–≥–æ –¥–∏–∞–ª–æ–≥–æ–≤: {total_dialogs}
- –í—Å–µ–≥–æ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π: {len(mentions)}
- –ù–∞–π–¥–µ–Ω–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {len(clusters)}

## –¢–µ–º—ã
"""
        
        for theme in themes_summary[:10]:  # –¢–æ–ø-10
            report_content += f"- {theme['theme']}: {theme['dialog_count']} –¥–∏–∞–ª–æ–≥–æ–≤ ({theme['share_of_dialogs_pct']}%)\n"
        
        report_content += "\n## –ü–æ–¥—Ç–µ–º—ã\n"
        for subtheme in subthemes_summary[:20]:  # –¢–æ–ø-20
            report_content += f"- {subtheme['theme']} / {subtheme['subtheme']}: {subtheme['dialog_count']} –¥–∏–∞–ª–æ–≥–æ–≤\n"
        
        reports['main_report'] = report_content
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç
        with open('reports/dod_analysis_report.md', 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        logger.info("üìù –û—Ç—á–µ—Ç—ã —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã")
        return reports
    
    def save_results(self, output_dir: str = "artifacts"):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π
        with open(output_path / "mentions.jsonl", "w", encoding="utf-8") as f:
            for mention in self.results["mentions"]:
                f.write(json.dumps(mention, ensure_ascii=False) + "\n")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
        with open(output_path / "clusters.json", "w", encoding="utf-8") as f:
            json.dump(self.results["clusters"], f, ensure_ascii=False, indent=2)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–≤–æ–¥–æ–∫
        with open(output_path / "summaries.json", "w", encoding="utf-8") as f:
            json.dump(self.results["summaries"], f, ensure_ascii=False, indent=2)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∫–∞—á–µ—Å—Ç–≤–∞
        with open(output_path / "quality_results.json", "w", encoding="utf-8") as f:
            json.dump(self.results["quality_results"], f, ensure_ascii=False, indent=2)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        with open(output_path / "statistics.json", "w", encoding="utf-8") as f:
            json.dump(self.results["statistics"], f, ensure_ascii=False, indent=2)
        
        logger.info(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {output_dir}")

def load_dialogs_from_file(file_path: str) -> List[Dict[str, Any]]:
    """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∏–∞–ª–æ–≥–æ–≤ –∏–∑ —Ñ–∞–π–ª–∞"""
    file_path = Path(file_path)
    
    if file_path.suffix == '.xlsx':
        df = pd.read_excel(file_path)
        dialogs = []
        for idx, row in df.iterrows():
            dialog = {
                "dialog_id": idx,
                "turns": [
                    {"role": "client", "text": str(row.iloc[0]) if len(row) > 0 else ""}
                ]
            }
            dialogs.append(dialog)
        return dialogs
    elif file_path.suffix == '.json':
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        if isinstance(data, list):
            return data
        else:
            return data.get('dialogs', [])
    else:
        # –ü—Ä–æ—Å—Ç–æ–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç
        with open(file_path, 'r', encoding='utf-8') as f:
            lines = [line.strip() for line in f if line.strip()]
        dialogs = []
        for idx, line in enumerate(lines):
            dialogs.append({
                "dialog_id": idx,
                "turns": [{"role": "client", "text": line}]
            })
        return dialogs

async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    parser = argparse.ArgumentParser(description='–ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π DoD –ø–∞–π–ø–ª–∞–π–Ω')
    parser.add_argument('--input', '-i', required=True, help='–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å –¥–∏–∞–ª–æ–≥–∞–º–∏')
    parser.add_argument('--output', '-o', default='artifacts', help='–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤')
    parser.add_argument('--config', '-c', default='final_pipeline_config.json', help='–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è')
    
    args = parser.parse_args()
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∏–∞–ª–æ–≥–æ–≤
    logger.info(f"üìÇ –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∏–∞–ª–æ–≥–∏ –∏–∑ {args.input}")
    dialogs = load_dialogs_from_file(args.input)
    logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(dialogs)} –¥–∏–∞–ª–æ–≥–æ–≤")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∏ –∑–∞–ø—É—Å–∫ –ø–∞–π–ø–ª–∞–π–Ω–∞
    pipeline = IntegratedDoDPipeline(args.config)
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∏–∞–ª–æ–≥–æ–≤
    results = await pipeline.process_dialogs(dialogs)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    pipeline.save_results(args.output)
    
    # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ DoD
    quality = results["quality_results"]
    print("\n" + "="*50)
    print("üéØ –†–ï–ó–£–õ–¨–¢–ê–¢–´ DoD –ü–†–û–í–ï–†–û–ö")
    print("="*50)
    print(f"Evidence-100: {'‚úÖ' if quality.get('dod_status', {}).get('evidence_100') else '‚ùå'}")
    print(f"Client-only-100: {'‚úÖ' if quality.get('dod_status', {}).get('client_only_100') else '‚ùå'}")
    print(f"Dedup ‚â§1%: {'‚úÖ' if quality.get('dod_status', {}).get('dedup_1pct') else '‚ùå'}")
    print(f"Coverage ‚â•98%: {'‚úÖ' if quality.get('dod_status', {}).get('coverage_98pct') else '‚ùå'}")
    print(f"–û–±—â–∏–π —Å—Ç–∞—Ç—É—Å DoD: {'‚úÖ –ü–†–û–ô–î–ï–ù' if quality.get('dod_passed') else '‚ùå –ù–ï –ü–†–û–ô–î–ï–ù'}")
    print("="*50)
    
    logger.info("üéâ –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π DoD –ø–∞–π–ø–ª–∞–π–Ω –∑–∞–≤–µ—Ä—à–µ–Ω!")

if __name__ == "__main__":
    asyncio.run(main())
