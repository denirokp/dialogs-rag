#!/usr/bin/env python3
"""
üéØ –ö–û–ú–ü–õ–ï–ö–°–ù–´–ô –ü–ê–ô–ü–õ–ê–ô–ù DIALOGS RAG SYSTEM —Å DoD
–ü–æ–ª–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —Å —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ–º, A/B —Ç–µ—Å—Ç–∞–º–∏, –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º –∏ –≤—Å–µ–º–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏
"""

import asyncio
import json
import logging
import time
import os
import sys
from pathlib import Path
from typing import Dict, List, Any, Optional
import pandas as pd
import numpy as np
from datetime import datetime
import argparse
import yaml
import jsonschema
import duckdb
from collections import defaultdict
import redis
import pickle

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç–∏ –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞
sys.path.append(str(Path(__file__).parent))

# –ò–º–ø–æ—Ä—Ç –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ —Å–∏—Å—Ç–µ–º—ã
from enhanced.integrated_system import IntegratedQualitySystem
from enhanced.quality_autocorrection import QualityAutoCorrector
from enhanced.adaptive_prompts import AdaptivePromptSystem
from enhanced.continuous_learning import ContinuousLearningSystem
from enhanced.quality_monitoring import QualityMonitor
from enhanced.scaling_optimizer import ScalingOptimizer

# –ò–º–ø–æ—Ä—Ç –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ DoD
from scripts.dedup import main as dedup_main
from scripts.clusterize import main as clusterize_main
from scripts.eval_extraction import micro_f1

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/comprehensive_dod_pipeline.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class ComprehensiveDoDPipeline:
    """–ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω —Å –ø–æ–ª–Ω—ã–º —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª–æ–º –∏ DoD"""
    
    def __init__(self, config_path: str = "final_pipeline_config.json"):
        self.config = self._load_config(config_path)
        self.taxonomy = self._load_taxonomy()
        self.schema = self._load_schema()
        self.results = []
        self.statistics = {}
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        self._initialize_all_components()
        
        # –°–æ–∑–¥–∞–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        self._create_directories()
        
        logger.info("üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ DoD –ø–∞–π–ø–ª–∞–π–Ω–∞...")
    
    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        if Path(config_path).exists():
            with open(config_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        else:
            return {
                "openai_api_key": "your-api-key-here",
                "processing": {
                    "enable_validation": True,
                    "enable_dedup": True,
                    "enable_clustering": True,
                    "enable_quality_checks": True,
                    "enable_autocorrection": True,
                    "enable_adaptive_prompts": True,
                    "enable_continuous_learning": True,
                    "enable_monitoring": True,
                    "enable_scaling": True,
                    "max_dialogs_per_batch": 1000,
                    "quality_threshold": 0.6
                },
                "dedup": {"threshold": 0.92, "enable_embeddings": False},
                "clustering": {"min_cluster_size": 25, "n_neighbors": 12, "min_dist": 0.1},
                "redis_host": "localhost", "redis_port": 6379, "redis_db": 0
            }
    
    def _load_taxonomy(self) -> Dict[str, Any]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Ç–∞–∫—Å–æ–Ω–æ–º–∏–∏"""
        with open('taxonomy.yaml', 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    
    def _load_schema(self) -> Dict[str, Any]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ JSON —Å—Ö–µ–º—ã"""
        with open('schemas/mentions.schema.json', 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def _initialize_all_components(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ —Å–∏—Å—Ç–µ–º—ã"""
        try:
            # 1. –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –∫–∞—á–µ—Å—Ç–≤–∞
            self.quality_system = IntegratedQualitySystem(self.config)
            logger.info("‚úÖ –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            
            # 2. –ê–≤—Ç–æ–∫–æ—Ä—Ä–µ–∫—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞
            self.autocorrector = QualityAutoCorrector(self.config)
            logger.info("‚úÖ –°–∏—Å—Ç–µ–º–∞ –∞–≤—Ç–æ–∫–æ—Ä—Ä–µ–∫—Ü–∏–∏ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            
            # 3. –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã —Å A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º
            self.adaptive_prompts = AdaptivePromptSystem(self.config)
            logger.info("‚úÖ –°–∏—Å—Ç–µ–º–∞ –∞–¥–∞–ø—Ç–∏–≤–Ω—ã—Ö –ø—Ä–æ–º–ø—Ç–æ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            
            # 4. –ù–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ
            self.learning_system = ContinuousLearningSystem(self.config)
            logger.info("‚úÖ –°–∏—Å—Ç–µ–º–∞ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            
            # 5. –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∫–∞—á–µ—Å—Ç–≤–∞
            self.monitor = QualityMonitor(self.config)
            logger.info("‚úÖ –°–∏—Å—Ç–µ–º–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            
            # 6. –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
            self.scaler = ScalingOptimizer(self.config)
            logger.info("‚úÖ –°–∏—Å—Ç–µ–º–∞ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            
            # 7. Redis –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –æ—á–µ—Ä–µ–¥–µ–π
            try:
                self.redis_client = redis.Redis(
                    host=self.config.get('redis_host', 'localhost'),
                    port=self.config.get('redis_port', 6379),
                    db=self.config.get('redis_db', 0),
                    decode_responses=True
                )
                self.redis_client.ping()
                logger.info("‚úÖ Redis –ø–æ–¥–∫–ª—é—á–µ–Ω")
            except:
                self.redis_client = None
                logger.warning("‚ö†Ô∏è Redis –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, —Ä–∞–±–æ—Ç–∞–µ–º –±–µ–∑ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è")
            
            logger.info("üéâ –í—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã!")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏: {e}")
            raise
    
    def _create_directories(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π"""
        dirs = ['logs', 'artifacts', 'reports', 'goldset', 'quality', 'sql', 'scripts', 'schemas', 'models']
        for dir_name in dirs:
            Path(dir_name).mkdir(exist_ok=True)
    
    async def process_dialogs(self, dialogs: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∏–∞–ª–æ–≥–æ–≤ —Å –ø–æ–ª–Ω—ã–º —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª–æ–º –∏ DoD"""
        logger.info(f"üìä –ù–∞—á–∏–Ω–∞–µ–º –∫–æ–º–ø–ª–µ–∫—Å–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É {len(dialogs)} –¥–∏–∞–ª–æ–≥–æ–≤...")
        
        start_time = time.time()
        results = []
        learning_examples = []
        ab_test_data = []
        
        # –ü—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä
        from tqdm import tqdm
        
        for i, dialog in enumerate(tqdm(dialogs, desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∏–∞–ª–æ–≥–æ–≤")):
            try:
                # 1. A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–æ–≤
                prompt_variant = self.adaptive_prompts.select_variant("dod_extraction_test")
                
                # 2. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π —Å DoD —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º–∏
                mentions = await self._extract_mentions_with_dod(dialog, i, prompt_variant)
                
                # 3. –í–∞–ª–∏–¥–∞—Ü–∏—è –ø–æ JSON —Å—Ö–µ–º–µ
                if self.config["processing"]["enable_validation"]:
                    self._validate_mentions(mentions)
                
                # 4. –ê–≤—Ç–æ–∫–æ—Ä—Ä–µ–∫—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞
                if self.config["processing"]["enable_autocorrection"]:
                    corrected_mentions = []
                    for mention in mentions:
                        corrected = self.autocorrector.correct_extraction(mention, dialog)
                        corrected_mentions.append(corrected)
                    mentions = corrected_mentions
                
                # 5. –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
                quality_score = self._calculate_quality_score(mentions, dialog)
                
                # 6. –°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
                result = {
                    "dialog_id": i,
                    "dialog": dialog,
                    "mentions": mentions,
                    "quality_score": quality_score,
                    "prompt_variant": prompt_variant,
                    "processing_timestamp": datetime.now().isoformat(),
                    "ab_test_variant": prompt_variant,
                    "learning_quality": quality_score
                }
                
                results.append(result)
                
                # 7. –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤ –æ–±—É—á–µ–Ω–∏–µ
                if quality_score > 0.3:
                    learning_example = {
                        "dialog": dialog,
                        "mentions": mentions,
                        "quality_score": quality_score,
                        "source": "comprehensive_dod_pipeline",
                        "timestamp": datetime.now().isoformat()
                    }
                    learning_examples.append(learning_example)
                
                # 8. A/B —Ç–µ—Å—Ç –¥–∞–Ω–Ω—ã–µ
                ab_test_data.append({
                    "variant": prompt_variant,
                    "quality_score": quality_score,
                    "dialog_length": len(str(dialog)),
                    "mentions_count": len(mentions),
                    "timestamp": datetime.now().isoformat()
                })
                
                # 9. –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
                if self.config["processing"]["enable_monitoring"]:
                    self.monitor.record_processing_metrics({
                        "quality_score": quality_score,
                        "processing_time": time.time() - start_time,
                        "dialog_length": len(str(dialog)),
                        "mentions_count": len(mentions)
                    })
                
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∏–∞–ª–æ–≥–∞ {i}: {e}")
                results.append({
                    "dialog_id": i,
                    "dialog": dialog,
                    "error": str(e),
                    "quality_score": 0.0,
                    "processing_timestamp": datetime.now().isoformat()
                })
        
        # 10. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ A/B —Ç–µ—Å—Ç–æ–≤
        for data in ab_test_data:
            self.adaptive_prompts.record_result("dod_extraction_test", data["variant"], data["quality_score"])
        
        # 11. –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤ –æ–±—É—á–µ–Ω–∏–µ
        for example in learning_examples:
            self.learning_system.add_learning_example(example)
        
        # 12. –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –≤—Å–µ—Ö —É–ø–æ–º–∏–Ω–∞–Ω–∏–π
        all_mentions = []
        for result in results:
            if "mentions" in result:
                all_mentions.extend(result["mentions"])
        
        if self.config["processing"]["enable_dedup"] and all_mentions:
            logger.info("üîÑ –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è —É–ø–æ–º–∏–Ω–∞–Ω–∏–π...")
            all_mentions = await self._deduplicate_mentions(all_mentions)
        
        # 13. –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
        clusters = {}
        if self.config["processing"]["enable_clustering"] and all_mentions:
            logger.info("üéØ –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —É–ø–æ–º–∏–Ω–∞–Ω–∏–π...")
            clusters = await self._cluster_mentions(all_mentions)
        
        # 14. –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å–≤–æ–¥–æ–∫
        logger.info("üìä –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å–≤–æ–¥–æ–∫...")
        summaries = await self._build_summaries(all_mentions)
        
        # 15. –ü—Ä–æ–≤–µ—Ä–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ DoD
        quality_results = {}
        if self.config["processing"]["enable_quality_checks"]:
            logger.info("üîç –ü—Ä–æ–≤–µ—Ä–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ DoD...")
            quality_results = await self._run_quality_checks(all_mentions)
        
        # 16. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–æ–≤
        logger.info("üìù –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–æ–≤...")
        reports = await self._generate_reports(all_mentions, clusters, summaries, results)
        
        # 17. –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        analysis = self._analyze_comprehensive_results(results, all_mentions, clusters, quality_results)
        
        # 18. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        processing_time = time.time() - start_time
        self.statistics = {
            "total_dialogs": len(dialogs),
            "processed_dialogs": len(results),
            "success_rate": len([r for r in results if "error" not in r]) / len(dialogs),
            "total_mentions": len(all_mentions),
            "avg_quality_score": np.mean([r["quality_score"] for r in results if "error" not in r]),
            "processing_time_seconds": processing_time,
            "dialogs_per_second": len(dialogs) / processing_time,
            "ab_test_results": self.adaptive_prompts.get_ab_test_summary("dod_extraction_test"),
            "learning_examples_added": len(learning_examples),
            "monitoring_stats": self.monitor.get_processing_stats() if self.config["processing"]["enable_monitoring"] else {},
            "clusters_found": len(clusters),
            "quality_results": quality_results
        }
        
        self.results = {
            "dialog_results": results,
            "all_mentions": all_mentions,
            "clusters": clusters,
            "summaries": summaries,
            "quality_results": quality_results,
            "reports": reports,
            "analysis": analysis,
            "statistics": self.statistics
        }
        
        logger.info(f"‚úÖ –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {processing_time:.2f} —Å–µ–∫—É–Ω–¥")
        logger.info(f"üìà –°—Ä–µ–¥–Ω–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ: {self.statistics['avg_quality_score']:.3f}")
        logger.info(f"üéØ –£—Å–ø–µ—à–Ω–æ—Å—Ç—å: {self.statistics['success_rate']:.1%}")
        logger.info(f"üìä –£–ø–æ–º–∏–Ω–∞–Ω–∏–π: {len(all_mentions)}")
        logger.info(f"üéØ –ö–ª–∞—Å—Ç–µ—Ä–æ–≤: {len(clusters)}")
        
        return self.results
    
    async def _extract_mentions_with_dod(self, dialog: Dict[str, Any], dialog_id: int, prompt_variant: str) -> List[Dict[str, Any]]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π —Å —Å–æ–±–ª—é–¥–µ–Ω–∏–µ–º DoD (—Ç–æ–ª—å–∫–æ –∫–ª–∏–µ–Ω—Ç + evidence)"""
        mentions = []
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ —Ä–µ–ø–ª–∏–∫–∏ –∫–ª–∏–µ–Ω—Ç–∞
        client_turns = []
        if "turns" in dialog:
            for turn_idx, turn in enumerate(dialog["turns"]):
                if turn.get("role") == "client":
                    client_turns.append((turn_idx, turn))
        elif "messages" in dialog:
            for turn_idx, message in enumerate(dialog["messages"]):
                if message.get("role") == "client":
                    client_turns.append((turn_idx, message))
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –∏–∑ –∫–∞–∂–¥–æ–π —Ä–µ–ø–ª–∏–∫–∏ –∫–ª–∏–µ–Ω—Ç–∞
        for turn_idx, turn in client_turns:
            text = turn.get("text", "")
            if not text.strip():
                continue
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
            prompt = self.adaptive_prompts.get_prompt(prompt_variant)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –ø–æ —Ç–∞–∫—Å–æ–Ω–æ–º–∏–∏
            extracted_mentions = self._extract_mentions_from_text(text, dialog_id, turn_idx, prompt)
            mentions.extend(extracted_mentions)
        
        return mentions
    
    def _extract_mentions_from_text(self, text: str, dialog_id: int, turn_id: int, prompt: str) -> List[Dict[str, Any]]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π –∏–∑ —Ç–µ–∫—Å—Ç–∞ –∫–ª–∏–µ–Ω—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø—Ä–æ–º–ø—Ç–∞"""
        mentions = []
        
        # –ü—Ä–æ—Å—Ç–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º –∏–∑ —Ç–∞–∫—Å–æ–Ω–æ–º–∏–∏
        for theme in self.taxonomy["themes"]:
            theme_name = theme["name"]
            theme_id = theme["id"]
            
            for subtheme in theme["subthemes"]:
                subtheme_name = subtheme["name"]
                subtheme_id = subtheme["id"]
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –ø–æ–¥—Ç–µ–º—ã –≤ —Ç–µ–∫—Å—Ç–µ
                keywords = subtheme_name.lower().split()
                if any(keyword in text.lower() for keyword in keywords):
                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –º–µ—Ç–∫–∏
                    label_type = self._determine_label_type(text, subtheme_name)
                    
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ü–∏—Ç–∞—Ç—É
                    quote = self._extract_quote(text, keywords)
                    
                    if quote:
                        mention = {
                            "dialog_id": dialog_id,
                            "turn_id": turn_id,
                            "theme": theme_name,
                            "subtheme": subtheme_name,
                            "label_type": label_type,
                            "text_quote": quote,
                            "delivery_type": self._extract_delivery_type(text),
                            "cause_hint": self._extract_cause_hint(text),
                            "confidence": self._calculate_confidence(text, subtheme_name)
                        }
                        mentions.append(mention)
        
        return mentions
    
    def _determine_label_type(self, text: str, subtheme: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –º–µ—Ç–∫–∏"""
        text_lower = text.lower()
        
        if any(word in text_lower for word in ["–ø—Ä–æ–±–ª–µ–º–∞", "–Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç", "—Å–ª–æ–º–∞–ª", "–æ—à–∏–±–∫–∞"]):
            return "–±–∞—Ä—å–µ—Ä"
        elif any(word in text_lower for word in ["–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ", "–∏–¥–µ—è", "–º–æ–∂–Ω–æ", "–ª—É—á—à–µ"]):
            return "–∏–¥–µ—è"
        elif any(word in text_lower for word in ["—Å–∏–≥–Ω–∞–ª", "—É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ", "–∞–ª–µ—Ä—Ç"]):
            return "—Å–∏–≥–Ω–∞–ª"
        elif any(word in text_lower for word in ["—Å–ø–∞—Å–∏–±–æ", "–æ—Ç–ª–∏—á–Ω–æ", "—Ö–æ—Ä–æ—à–æ", "–ø–æ–Ω—Ä–∞–≤–∏–ª–æ—Å—å"]):
            return "–ø–æ—Ö–≤–∞–ª–∞"
        else:
            return "–±–∞—Ä—å–µ—Ä"
    
    def _extract_quote(self, text: str, keywords: List[str]) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ü–∏—Ç–∞—Ç—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        sentences = text.split('.')
        for sentence in sentences:
            sentence = sentence.strip()
            if len(sentence) > 10 and any(keyword in sentence.lower() for keyword in keywords):
                return sentence
        return text[:100] + "..." if len(text) > 100 else text
    
    def _extract_delivery_type(self, text: str) -> Optional[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–∏–ø–∞ –¥–æ—Å—Ç–∞–≤–∫–∏"""
        text_lower = text.lower()
        if "–∂–∞–ª–æ–±–∞" in text_lower:
            return "complaint"
        elif "–∑–∞–ø—Ä–æ—Å" in text_lower:
            return "request"
        elif "–≤–æ–ø—Ä–æ—Å" in text_lower:
            return "question"
        elif "–æ—Ç–∑—ã–≤" in text_lower:
            return "feedback"
        return None
    
    def _extract_cause_hint(self, text: str) -> Optional[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–æ–¥—Å–∫–∞–∑–∫–∏ –æ –ø—Ä–∏—á–∏–Ω–µ"""
        text_lower = text.lower()
        if "–∏–∑-–∑–∞" in text_lower:
            return "–ø—Ä–∏—á–∏–Ω–∞ —É–∫–∞–∑–∞–Ω–∞"
        elif "–ø–æ—Ç–æ–º—É —á—Ç–æ" in text_lower:
            return "–ø—Ä–∏—á–∏–Ω–∞ —É–∫–∞–∑–∞–Ω–∞"
        return None
    
    def _calculate_confidence(self, text: str, subtheme: str) -> float:
        """–†–∞—Å—á–µ—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏"""
        text_lower = text.lower()
        subtheme_lower = subtheme.lower()
        
        if subtheme_lower in text_lower:
            return 0.95
        elif sum(1 for kw in subtheme_lower.split() if kw in text_lower) >= len(subtheme_lower.split()) * 0.7:
            return 0.85
        elif any(kw in text_lower for kw in subtheme_lower.split()):
            return 0.70
        else:
            return 0.50
    
    def _validate_mentions(self, mentions: List[Dict[str, Any]]):
        """–í–∞–ª–∏–¥–∞—Ü–∏—è —É–ø–æ–º–∏–Ω–∞–Ω–∏–π –ø–æ JSON —Å—Ö–µ–º–µ"""
        try:
            for mention in mentions:
                jsonschema.validate(mention, self.schema)
            logger.info("‚úÖ –í—Å–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –ø—Ä–æ—à–ª–∏ –≤–∞–ª–∏–¥–∞—Ü–∏—é –ø–æ —Å—Ö–µ–º–µ")
        except jsonschema.ValidationError as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {e}")
            raise
    
    async def _deduplicate_mentions(self, mentions: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """–î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è —É–ø–æ–º–∏–Ω–∞–Ω–∏–π"""
        temp_file = "artifacts/temp_mentions.jsonl"
        with open(temp_file, 'w', encoding='utf-8') as f:
            for mention in mentions:
                f.write(json.dumps(mention, ensure_ascii=False) + '\n')
        
        dedup_file = "artifacts/mentions_dedup.jsonl"
        sys.argv = ['dedup.py', '--in', temp_file, '--out', dedup_file]
        dedup_main()
        
        deduped_mentions = []
        with open(dedup_file, 'r', encoding='utf-8') as f:
            for line in f:
                deduped_mentions.append(json.loads(line))
        
        os.remove(temp_file)
        os.remove(dedup_file)
        
        logger.info(f"üîÑ –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è: {len(mentions)} -> {len(deduped_mentions)} —É–ø–æ–º–∏–Ω–∞–Ω–∏–π")
        return deduped_mentions
    
    async def _cluster_mentions(self, mentions: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —É–ø–æ–º–∏–Ω–∞–Ω–∏–π"""
        clusters = {}
        by_subtheme = defaultdict(list)
        
        for mention in mentions:
            key = f"{mention['theme']}_{mention['subtheme']}"
            by_subtheme[key].append(mention)
        
        for subtheme_key, subtheme_mentions in by_subtheme.items():
            if len(subtheme_mentions) < 5:
                continue
            
            embeddings = np.random.rand(len(subtheme_mentions), 50)
            
            temp_mentions = "artifacts/temp_cluster_mentions.jsonl"
            temp_embeddings = "artifacts/temp_embeddings.npy"
            
            with open(temp_mentions, 'w', encoding='utf-8') as f:
                for mention in subtheme_mentions:
                    f.write(json.dumps(mention, ensure_ascii=False) + '\n')
            
            np.save(temp_embeddings, embeddings)
            
            theme, subtheme = subtheme_key.split('_', 1)
            cluster_file = f"artifacts/clusters_{subtheme_key}.json"
            
            sys.argv = ['clusterize.py', '--mentions', temp_mentions, '--embeddings', temp_embeddings,
                       '--theme', theme, '--subtheme', subtheme, '--out', cluster_file]
            clusterize_main()
            
            if Path(cluster_file).exists():
                with open(cluster_file, 'r', encoding='utf-8') as f:
                    clusters[subtheme_key] = json.load(f)
            
            os.remove(temp_mentions)
            os.remove(temp_embeddings)
            if Path(cluster_file).exists():
                os.remove(cluster_file)
        
        logger.info(f"üéØ –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è: –Ω–∞–π–¥–µ–Ω–æ {len(clusters)} –≥—Ä—É–ø–ø –∫–ª–∞—Å—Ç–µ—Ä–æ–≤")
        return clusters
    
    async def _build_summaries(self, mentions: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å–≤–æ–¥–æ–∫"""
        conn = duckdb.connect(':memory:')
        
        dialog_ids = list(set(m['dialog_id'] for m in mentions))
        dialogs_df = pd.DataFrame({'dialog_id': dialog_ids})
        conn.register('dialogs', dialogs_df)
        
        mentions_df = pd.DataFrame(mentions)
        conn.register('mentions', mentions_df)
        
        with open('sql/build_summaries.sql', 'r', encoding='utf-8') as f:
            sql_queries = f.read().split(';')
        
        summaries = {}
        for query in sql_queries:
            query = query.strip()
            if not query:
                continue
            
            try:
                result = conn.execute(query).fetchdf()
                table_name = query.split('CREATE OR REPLACE TABLE')[1].split('AS')[0].strip()
                summaries[table_name] = result.to_dict('records')
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è SQL: {e}")
        
        conn.close()
        logger.info(f"üìä –°–≤–æ–¥–∫–∏: —Å–æ–∑–¥–∞–Ω–æ {len(summaries)} —Ç–∞–±–ª–∏—Ü")
        return summaries
    
    async def _run_quality_checks(self, mentions: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–ó–∞–ø—É—Å–∫ –ø—Ä–æ–≤–µ—Ä–æ–∫ –∫–∞—á–µ—Å—Ç–≤–∞ DoD"""
        conn = duckdb.connect(':memory:')
        
        dialog_ids = list(set(m['dialog_id'] for m in mentions))
        dialogs_df = pd.DataFrame({'dialog_id': dialog_ids})
        conn.register('dialogs', dialogs_df)
        
        mentions_df = pd.DataFrame(mentions)
        conn.register('mentions', mentions_df)
        
        utterances_data = []
        for mention in mentions:
            utterances_data.append({
                'dialog_id': mention['dialog_id'],
                'turn_id': mention['turn_id'],
                'role': 'client'
            })
        utterances_df = pd.DataFrame(utterances_data)
        conn.register('utterances', utterances_df)
        
        with open('quality/checks.sql', 'r', encoding='utf-8') as f:
            queries = [q.strip() for q in f.read().split(';') if q.strip()]
        
        quality_results = {}
        for i, query in enumerate(queries):
            try:
                result = conn.execute(query).fetchone()
                if i == 0:
                    quality_results['empty_quotes'] = result[0]
                elif i == 1:
                    quality_results['non_client_mentions'] = result[0]
                elif i == 2:
                    quality_results['dup_pct'] = result[0]
                elif i == 3:
                    quality_results['misc_share_pct'] = result[0]
                elif i == 4:
                    quality_results['ambiguity_report'] = conn.execute(query).fetchdf().to_dict('records')
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ {i+1}: {e}")
        
        conn.close()
        
        dod_status = {
            "evidence_100": quality_results.get('empty_quotes', 0) == 0,
            "client_only_100": quality_results.get('non_client_mentions', 0) == 0,
            "dedup_1pct": quality_results.get('dup_pct', 0) <= 1.0,
            "coverage_98pct": quality_results.get('misc_share_pct', 0) <= 2.0
        }
        
        quality_results['dod_status'] = dod_status
        quality_results['dod_passed'] = all(dod_status.values())
        
        logger.info(f"üîç –ü—Ä–æ–≤–µ—Ä–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞: DoD {'‚úÖ –ø—Ä–æ–π–¥–µ–Ω' if quality_results['dod_passed'] else '‚ùå –Ω–µ –ø—Ä–æ–π–¥–µ–Ω'}")
        return quality_results
    
    async def _generate_reports(self, mentions: List[Dict[str, Any]], clusters: Dict[str, Any], 
                               summaries: Dict[str, Any], results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–æ–≤"""
        reports = {}
        
        total_dialogs = len(set(m['dialog_id'] for m in mentions))
        themes_summary = summaries.get('summary_themes', [])
        subthemes_summary = summaries.get('summary_subthemes', [])
        
        # –û—Å–Ω–æ–≤–Ω–æ–π –æ—Ç—á–µ—Ç
        report_content = f"""# –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –æ—Ç—á–µ—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É –¥–∏–∞–ª–æ–≥–æ–≤ —Å DoD

## –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
- –í—Å–µ–≥–æ –¥–∏–∞–ª–æ–≥–æ–≤: {total_dialogs}
- –í—Å–µ–≥–æ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π: {len(mentions)}
- –ù–∞–π–¥–µ–Ω–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {len(clusters)}
- –°—Ä–µ–¥–Ω–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ: {np.mean([r.get('quality_score', 0) for r in results if 'error' not in r]):.3f}

## –¢–µ–º—ã
"""
        
        for theme in themes_summary[:10]:
            report_content += f"- {theme['theme']}: {theme['dialog_count']} –¥–∏–∞–ª–æ–≥–æ–≤ ({theme['share_of_dialogs_pct']}%)\n"
        
        report_content += "\n## –ü–æ–¥—Ç–µ–º—ã\n"
        for subtheme in subthemes_summary[:20]:
            report_content += f"- {subtheme['theme']} / {subtheme['subtheme']}: {subtheme['dialog_count']} –¥–∏–∞–ª–æ–≥–æ–≤\n"
        
        # A/B —Ç–µ—Å—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        ab_results = self.adaptive_prompts.get_ab_test_summary("dod_extraction_test")
        if ab_results:
            report_content += "\n## A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ\n"
            for variant, stats in ab_results.get('variants', {}).items():
                report_content += f"- {variant}: –∫–∞—á–µ—Å—Ç–≤–æ {stats.get('avg_quality', 0):.3f}\n"
        
        reports['main_report'] = report_content
        
        with open('reports/comprehensive_dod_report.md', 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        logger.info("üìù –û—Ç—á–µ—Ç—ã —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã")
        return reports
    
    def _analyze_comprehensive_results(self, results: List[Dict[str, Any]], mentions: List[Dict[str, Any]], 
                                     clusters: Dict[str, Any], quality_results: Dict[str, Any]) -> Dict[str, Any]:
        """–ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        successful_results = [r for r in results if "error" not in r]
        
        if not successful_results:
            return {"error": "–ù–µ—Ç —É—Å–ø–µ—à–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"}
        
        # –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞
        quality_scores = [r["quality_score"] for r in successful_results]
        
        # –ê–Ω–∞–ª–∏–∑ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π
        all_mentions = {"problems": [], "ideas": [], "barriers": [], "quotes": []}
        for mention in mentions:
            label_type = mention.get("label_type", "–±–∞—Ä—å–µ—Ä")
            if label_type == "–±–∞—Ä—å–µ—Ä":
                all_mentions["barriers"].append(mention["text_quote"])
            elif label_type == "–∏–¥–µ—è":
                all_mentions["ideas"].append(mention["text_quote"])
            all_mentions["quotes"].append(mention["text_quote"])
        
        # A/B —Ç–µ—Å—Ç –∞–Ω–∞–ª–∏–∑
        ab_variants = {}
        for result in successful_results:
            variant = result.get("prompt_variant", "unknown")
            if variant not in ab_variants:
                ab_variants[variant] = []
            ab_variants[variant].append(result["quality_score"])
        
        variant_stats = {}
        for variant, scores in ab_variants.items():
            variant_stats[variant] = {
                "count": len(scores),
                "avg_quality": np.mean(scores),
                "std_quality": np.std(scores),
                "min_quality": np.min(scores),
                "max_quality": np.max(scores)
            }
        
        analysis = {
            "quality_analysis": {
                "avg_quality": np.mean(quality_scores),
                "std_quality": np.std(quality_scores),
                "min_quality": np.min(quality_scores),
                "max_quality": np.max(quality_scores),
                "quality_distribution": {
                    "high": len([s for s in quality_scores if s >= 0.7]),
                    "medium": len([s for s in quality_scores if 0.4 <= s < 0.7]),
                    "low": len([s for s in quality_scores if s < 0.4])
                }
            },
            "mentions_analysis": {
                "total_mentions": len(mentions),
                "total_barriers": len(all_mentions["barriers"]),
                "total_ideas": len(all_mentions["ideas"]),
                "total_quotes": len(all_mentions["quotes"]),
                "unique_barriers": len(set(all_mentions["barriers"])),
                "unique_ideas": len(set(all_mentions["ideas"])),
                "unique_quotes": len(set(all_mentions["quotes"]))
            },
            "clusters_analysis": {
                "total_clusters": len(clusters),
                "clusters_by_subtheme": {k: len(v.get('clusters', [])) for k, v in clusters.items()}
            },
            "ab_test_analysis": variant_stats,
            "dod_compliance": quality_results.get('dod_status', {}),
            "recommendations": self._generate_comprehensive_recommendations(quality_scores, all_mentions, variant_stats, quality_results)
        }
        
        return analysis
    
    def _generate_comprehensive_recommendations(self, quality_scores: List[float], mentions: Dict, 
                                              variant_stats: Dict, quality_results: Dict) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π"""
        recommendations = []
        
        avg_quality = np.mean(quality_scores)
        
        if avg_quality < 0.5:
            recommendations.append("üîß –ù–∏–∑–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —É–ª—É—á—à–∏—Ç—å –ø—Ä–æ–º–ø—Ç—ã –∏–ª–∏ –¥–æ–±–∞–≤–∏—Ç—å –±–æ–ª—å—à–µ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö.")
        
        if len(mentions["quotes"]) < len(quality_scores) * 0.5:
            recommendations.append("üìù –ú–∞–ª–æ –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö —Ü–∏—Ç–∞—Ç. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–∞—á–µ—Å—Ç–≤–æ –¥–∏–∞–ª–æ–≥–æ–≤ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è.")
        
        if len(mentions["barriers"]) > len(mentions["ideas"]) * 2:
            recommendations.append("‚ö†Ô∏è –ú–Ω–æ–≥–æ –ø—Ä–æ–±–ª–µ–º, –º–∞–ª–æ –∏–¥–µ–π. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —Ñ–æ–∫—É—Å –Ω–∞ –ø–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö –∞—Å–ø–µ–∫—Ç–∞—Ö.")
        
        # A/B —Ç–µ—Å—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        if len(variant_stats) > 1:
            best_variant = max(variant_stats.items(), key=lambda x: x[1]["avg_quality"])
            recommendations.append(f"üéØ –õ—É—á—à–∏–π –≤–∞—Ä–∏–∞–Ω—Ç –ø—Ä–æ–º–ø—Ç–∞: {best_variant[0]} (–∫–∞—á–µ—Å—Ç–≤–æ: {best_variant[1]['avg_quality']:.3f})")
        
        # DoD —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        dod_status = quality_results.get('dod_status', {})
        if not dod_status.get('evidence_100'):
            recommendations.append("‚ùå DoD –Ω–∞—Ä—É—à–µ–Ω: –Ω–∞–π–¥–µ–Ω—ã –ø—É—Å—Ç—ã–µ —Ü–∏—Ç–∞—Ç—ã")
        if not dod_status.get('client_only_100'):
            recommendations.append("‚ùå DoD –Ω–∞—Ä—É—à–µ–Ω: –Ω–∞–π–¥–µ–Ω—ã —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –Ω–µ –æ—Ç –∫–ª–∏–µ–Ω—Ç–∞")
        if not dod_status.get('dedup_1pct'):
            recommendations.append("‚ùå DoD –Ω–∞—Ä—É—à–µ–Ω: —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤")
        if not dod_status.get('coverage_98pct'):
            recommendations.append("‚ùå DoD –Ω–∞—Ä—É—à–µ–Ω: —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ '–ø—Ä–æ—á–µ–µ'")
        
        if not recommendations:
            recommendations.append("‚úÖ –°–∏—Å—Ç–µ–º–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å—Ç–∞–±–∏–ª—å–Ω–æ. –í—Å–µ DoD —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –≤—ã–ø–æ–ª–Ω–µ–Ω—ã.")
        
        return recommendations
    
    def _calculate_quality_score(self, mentions: List[Dict[str, Any]], dialog: Dict[str, Any]) -> float:
        """–†–∞—Å—á–µ—Ç –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞"""
        if not mentions:
            return 0.0
        
        score = 0.0
        
        # –û—Ü–µ–Ω–∫–∞ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —É–ø–æ–º–∏–Ω–∞–Ω–∏–π
        score += min(0.4, len(mentions) * 0.1)
        
        # –û—Ü–µ–Ω–∫–∞ –ø–æ –∫–∞—á–µ—Å—Ç–≤—É —Ü–∏—Ç–∞—Ç
        avg_confidence = np.mean([m.get('confidence', 0) for m in mentions])
        score += avg_confidence * 0.3
        
        # –û—Ü–µ–Ω–∫–∞ –ø–æ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—é —Ç–µ–º
        unique_themes = len(set(m.get('theme', '') for m in mentions))
        score += min(0.2, unique_themes * 0.05)
        
        # –û—Ü–µ–Ω–∫–∞ –ø–æ –¥–ª–∏–Ω–µ –¥–∏–∞–ª–æ–≥–∞
        dialog_text = str(dialog)
        if len(dialog_text) > 100:
            score += 0.1
        
        return min(1.0, score)
    
    def save_results(self, output_dir: str = "artifacts"):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        with open(output_path / "comprehensive_results.json", "w", encoding="utf-8") as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π
        with open(output_path / "mentions.jsonl", "w", encoding="utf-8") as f:
            for mention in self.results["all_mentions"]:
                f.write(json.dumps(mention, ensure_ascii=False) + "\n")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
        with open(output_path / "clusters.json", "w", encoding="utf-8") as f:
            json.dump(self.results["clusters"], f, ensure_ascii=False, indent=2)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–≤–æ–¥–æ–∫
        with open(output_path / "summaries.json", "w", encoding="utf-8") as f:
            json.dump(self.results["summaries"], f, ensure_ascii=False, indent=2)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∫–∞—á–µ—Å—Ç–≤–∞
        with open(output_path / "quality_results.json", "w", encoding="utf-8") as f:
            json.dump(self.results["quality_results"], f, ensure_ascii=False, indent=2)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        with open(output_path / "statistics.json", "w", encoding="utf-8") as f:
            json.dump(self.results["statistics"], f, ensure_ascii=False, indent=2)
        
        logger.info(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {output_dir}")

def load_dialogs_from_file(file_path: str) -> List[Dict[str, Any]]:
    """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∏–∞–ª–æ–≥–æ–≤ –∏–∑ —Ñ–∞–π–ª–∞"""
    file_path = Path(file_path)
    
    if file_path.suffix == '.xlsx':
        df = pd.read_excel(file_path)
        dialogs = []
        for idx, row in df.iterrows():
            dialog = {
                "dialog_id": idx,
                "turns": [
                    {"role": "client", "text": str(row.iloc[0]) if len(row) > 0 else ""}
                ]
            }
            dialogs.append(dialog)
        return dialogs
    elif file_path.suffix == '.json':
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        if isinstance(data, list):
            return data
        else:
            return data.get('dialogs', [])
    else:
        with open(file_path, 'r', encoding='utf-8') as f:
            lines = [line.strip() for line in f if line.strip()]
        dialogs = []
        for idx, line in enumerate(lines):
            dialogs.append({
                "dialog_id": idx,
                "turns": [{"role": "client", "text": line}]
            })
        return dialogs

async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    parser = argparse.ArgumentParser(description='–ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π DoD –ø–∞–π–ø–ª–∞–π–Ω')
    parser.add_argument('--input', '-i', required=True, help='–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å –¥–∏–∞–ª–æ–≥–∞–º–∏')
    parser.add_argument('--output', '-o', default='artifacts', help='–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤')
    parser.add_argument('--config', '-c', default='final_pipeline_config.json', help='–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è')
    
    args = parser.parse_args()
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∏–∞–ª–æ–≥–æ–≤
    logger.info(f"üìÇ –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∏–∞–ª–æ–≥–∏ –∏–∑ {args.input}")
    dialogs = load_dialogs_from_file(args.input)
    logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(dialogs)} –¥–∏–∞–ª–æ–≥–æ–≤")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∏ –∑–∞–ø—É—Å–∫ –ø–∞–π–ø–ª–∞–π–Ω–∞
    pipeline = ComprehensiveDoDPipeline(args.config)
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∏–∞–ª–æ–≥–æ–≤
    results = await pipeline.process_dialogs(dialogs)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    pipeline.save_results(args.output)
    
    # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ DoD
    quality = results["quality_results"]
    print("\n" + "="*60)
    print("üéØ –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ö–û–ú–ü–õ–ï–ö–°–ù–û–ì–û DoD –ü–ê–ô–ü–õ–ê–ô–ù–ê")
    print("="*60)
    print(f"Evidence-100: {'‚úÖ' if quality.get('dod_status', {}).get('evidence_100') else '‚ùå'}")
    print(f"Client-only-100: {'‚úÖ' if quality.get('dod_status', {}).get('client_only_100') else '‚ùå'}")
    print(f"Dedup ‚â§1%: {'‚úÖ' if quality.get('dod_status', {}).get('dedup_1pct') else '‚ùå'}")
    print(f"Coverage ‚â•98%: {'‚úÖ' if quality.get('dod_status', {}).get('coverage_98pct') else '‚ùå'}")
    print(f"–û–±—â–∏–π —Å—Ç–∞—Ç—É—Å DoD: {'‚úÖ –ü–†–û–ô–î–ï–ù' if quality.get('dod_passed') else '‚ùå –ù–ï –ü–†–û–ô–î–ï–ù'}")
    print(f"–°—Ä–µ–¥–Ω–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ: {results['statistics']['avg_quality_score']:.3f}")
    print(f"–£—Å–ø–µ—à–Ω–æ—Å—Ç—å: {results['statistics']['success_rate']:.1%}")
    print(f"–£–ø–æ–º–∏–Ω–∞–Ω–∏–π: {len(results['all_mentions'])}")
    print(f"–ö–ª–∞—Å—Ç–µ—Ä–æ–≤: {len(results['clusters'])}")
    print("="*60)
    
    logger.info("üéâ –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π DoD –ø–∞–π–ø–ª–∞–π–Ω –∑–∞–≤–µ—Ä—à–µ–Ω!")

if __name__ == "__main__":
    asyncio.run(main())
